\documentclass{article}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amsmath}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[a4paper, top=3cm, bottom=3cm]{geometry}
\usepackage{authblk}
\usepackage{setspace}
\usepackage{csquotes}
\usepackage{booktabs}
%%%%%%%%%%%%%%%%%% My preamble %%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[numbers, sort&compress]{natbib}
%\setcitestyle{numbers,square,comma,aysep={,},yysep={,},notesep={,}}
%\bibliographystyle{unsrtnat}
%\bibliographystyle{IEEEtranSN}

%References
\usepackage[
    bibstyle=phys,
    biblabel=brackets,
    citestyle=numeric-comp,
    isbn=false,
    doi=false,
    sorting=none,
    url=false,
    defernumbers=true,
    bibencoding=utf8,
    backend=biber,
    %maxbibnames=3,maxcitenames=3,
    %minnames=3,
    maxnames=30,
    ]{biblatex}
\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}
\addbibresource[]{eccd_ref.bib}
\DeclareBibliographyCategory{fullcited}
\newcommand{\mybibexclude}[1]{\addtocategory{fullcited}{#1}}
\DeclareFieldFormat{titlecase}{\MakeCapital{#1}}
\DeclareFieldFormat{sentencecase}{\MakeSentenceCase{#1}}

\usepackage[font=small,labelfont=bf]{caption}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{listings}

\usepackage{hyperref}

\newcommand{\be}[1]{\begin{equation} \label{#1}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}[1]{\begin{eqnarray} \label{#1}}
\newcommand{\eea}{\end{eqnarray}}
\setlength\parindent{0pt}

\newcommand{\br}{{\bf r}}
\newcommand{\bh}{{\bf h}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bB}{{\bf B}}
\newcommand{\rd}{{\rm d}}
\newcommand{\eq}[1]{(\ref{#1})}
\newcommand{\bp}{{\bf p}}
\newcommand{\difp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iotabar}{\mbox{$\iota$\hspace{-0.365em}-}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\lstset{basicstyle=\ttfamily}
\newcommand{\vb}{\lstinline}
\newcommand{\vv}[1]{\texttt{\detokenize{#1}}}

\title{\textbf{Technical NEO-2 documentation\\June 2018, Updates 2021}}


\author[1]{Winfried~Kernbichler}
\author[1]{Sergei~Kasilov}
\author[1]{Gernot~Kapper}
\author[1]{Rico~Buchholz}
\affil[1]{Fusion@\"OAW, Institute of Theoretical and Computational Physics, Graz University of Technology, Petersgasse 16, 8010 Graz, Austria}
\renewcommand\Affilfont{\itshape\small}

\date{}

\begin{document}
\onehalfspacing

\maketitle

\section{Introduction}
This serves as documentation on how to do ECCD simulations with
\vv{neo-2}. As such it describes the required tasks, but not the
physical background.

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Short summary}
\subsection{Required input}
\begin{itemize}
  \item boozer file
  \item profile file (density, temperature, effective charge)
  \item neo.in
  \item create\_surfaces.in
  \item neo2.in
  \item (condor submit file|slurm submit file)
\end{itemize}

\subsection{Required steps}
\begin{itemize}
  \item get profiles (Sec.~\ref{running_preparation_profiles})
  \item get magnetic field as boozer file (Sec.~\ref{running_preparation_equilibrium})
  \item make sure you have all required libraries (Sec.~\ref{technicalbackground_libraries})
  \item clone/update \vv{libneo} and \vv{neo-2} repositories (Sec.~\ref{technicalbackground_git})
  \item compile the codes (Sec.~\ref{technicalbackground_cmake})
  \item create working directory, and a template directory in this (Sec.~\ref{running_preparation_directories})
  \item transform input into required form for create\_surfaces.x (Sec.~\ref{running_preparation_profiles})
  \item get/create configuration files (Sec.~\ref{running_preparation_neo2in})
  \item check/edit settings as necessary (Sec.~\ref{running_preparation_neo2in})
  \item run create\_surfaces.x (Sec.~\ref{running_preparation_directories})
  \item run create\_surf.py (Sec.~\ref{running_preparation_directories})
  \item run neo2.x for prop\_reconstruct=0 (Sec.~\ref{running_running})
  \item run neo2.x for prop\_reconstruct=1 (Sec.~\ref{running_running})
  \item run neo2.x for prop\_reconstruct=2 (Sec.~\ref{running_running})
  \item run neo2.x for prop\_reconstruct=3 (Sec.~\ref{running_running})
  \item run h5merge.x to collect data (Sec.~\ref{running_running})
  \item postprocessing (e.g. plotting) (Sec.~\ref{spitzer_function}, \ref{travis} and \ref{bootstrap})
\end{itemize}

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Technical background on NEO-2}
\subsection{Overview}
In the last years the development of \vv{neo-2} has split into two branches,
namely the more general branch for stellarators (and axisymmetric
tokamaks) and the quasilinear version for tokamaks with 3D magnetic
perturbations. The main differences between these two versions at time
of writing this document is given in the following Table~\ref{tab:neo2branches}.

\begin{table}[h]
\centering
\begin{tabular}{lll}
Internal name   & NEO-2-PAR & NEO-2-QL\\
Equilibrium     & Tokamak/Stellarator & Non-axisymmetric tokamak\\
Multispecies    & No & Yes\\
Relativistic    & Yes & Yes (but not for multispecies)\\
Parallelization & Yes (field line) & Yes (Species)\\
Solver          & $1^\mathrm{st}$ Order & $1^\mathrm{st}$ and $2^\mathrm{st}$ Order\\
Distribution function & Yes & No (planned)
\end{tabular}
\caption{Comparison of \vv{neo-2} branches}
\label{tab:neo2branches}
\end{table}

% ----------------------------------------------------------------------
\subsection{Libraries\label{technicalbackground_libraries}}
\vv{neo-2} depends on a number of external libraries. These libraries
are:
\begin{itemize}
\item SuiteSparse
\item Metis
\item SuperLU
\item GSL
\item FGSL
\item HDF5 - interfaced via hdf5tools
\item OpenMPI - interfaced via MyMPILib
\item libneo
\end{itemize}

SuiteSparse is used for solving the sparse linear system of differential
equations in the ripple solver. Metis is used by SuiteSparse for speedup
of some specific routines and is not obligatory. SuperLU can be used
instead of SuiteSparse by a switch in the input file.

The GSL (Gnu Scientific Library) and its Fortran Interface FGSL are used
for several purposes. The first one provides very efficient numerical
integration methods used for the computation of the matrix elements of
the collision operator in \verb|collop_compute.f90|. Additionally, GSL
provides B-spline routines, also used for the collision operator module
(basis functions). While writing this document FGSL is not distributed
with Debian, therefore this is build locally in \verb|/proj/plasma/Libs/|.
If you are sitting somewhere else, you might need to install it.

The HDF5 library is required for modern I/O of \vv{neo-2}. One advantage over
text files is that each variable (dataset) has a unique name and can
store additional attributes such as units and/or comments. The usage of HDF5
was unavoidable for storing the large datasets that occur when computing
the generalized Spitzer function for stellarators. In order to simplify
the calls a collection of wrapper routines has been created. This
project has grown over the time so that it was decided to use these
wrapper functions also in other projects, as in the interface, so that
it became an own library called \vv{hdf5tools}.

For parallelization the Message Passing Interface (MPI) is used. The
usage of MPI in \vv{neo-2} is performed via an own library, called MyMPILib,
and is not restricted to a particular MPI implementation. At our
institute it is linked against OpenMPI, where on clusters Intel MPI is
mostly used. MyMPILib was developed so that in the code of \vv{neo-2} no
native MPI commands have to be used.

Note that it was decided to no longer have \vv{hdf5tools} and
\vv{MyMPILib} as separate libraries, but to add them to \vv{libneo},
which is intended to collect code that is used by multiple programs of
the group. You can get \vv{libneo} from github (see
Sec.~\ref{technicalbackground_git}).

% ----------------------------------------------------------------------
\subsection{The Git repository\label{technicalbackground_git}}
Since 2013 the general version of \vv{neo-2} is under Git version control.
Later, also the quasilinear version was added to this repository
including its code history. The official repository is located on github:
\begin{verbatim}
https://github.com/itpplasma/NEO-2/
\end{verbatim}
At the moment it is set to private, which means you need to be logged in
to be able to see it.

In order to start a new local working copy, it is required to do
\begin{verbatim}
git clone git@github.com:itpplasma/NEO-2.git .
\end{verbatim}
in an empty directory (do not oversee the point at the end of the
command line which defines the current directory).

The same holds for \vv{libneo},
\begin{verbatim}
https://github.com/itpplasma/libneo/
\end{verbatim}
which is required by \vv{neo-2}.

% ----------------------------------------------------------------------
\subsection{CMake and Compiling \label{technicalbackground_cmake}}
The build system of \vv{neo-2} and \vv{libneo} is CMake (since 2012).
The main advantage is that this build system takes care of correct
compilation order of the source files (dependency resolving) and that
routines can be defined to automatically detect the installed
libraries. The configuration file is called \verb|CMakeLists.txt|. Using
CMake the source- and build-directories can be separated from each
other, and both codes require that this is done. Thus, in order to build
the source code it is necessary to create a build-directory. For reasons
of readability it was decided to put the source files to be compiled in
a separate file, called \vv{CMakeSources.in}. Additionaly, some of the
settings for paths are in a separate file, \vv{ProjectConfig.cmake.in}.
Mainly these settings are related to required libraries (see
sec.~\ref{technicalbackground_libraries}).
These files are then included in \vv{CMakeLists.txt}.

For testing purposes a build-directory is now created for \vv{libneo}:
\begin{verbatim}
mkdir Build-Test
cd Build-Test
cmake ..
make
\end{verbatim}

Running the command \verb|cmake ..| is only necessary the first time a
new build directory is used. This command needs to know where
\verb|CMakeLists.txt| is located (therefore the \verb|..|). A lot
happens when this command is called, the most important steps are the
Fortran compiler and library detection. A standard Makefile is generated
which can be used by running \vv{make}.

Before going on to compile \vv{neo-2} you should also compile the tool
\vv{h5merge} in the corresponding subfolder. The procedure to do so is
the same, just do it in the subfolder.

After libneo is compiled, you can compile \vv{neo-2}:
\begin{verbatim}
cd NEO-2-PAR
mkdir Build-Test
cd Build-Test
cmake ..
make
\end{verbatim}

The first step is required as the two versions of \vv{neo-2} have
separate folders and separate build-configuration files.

Also in this case there is a tool that should be compiled,
\vv{create_surfaces}.

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Running an ECCD precomputation}
\subsection{Preparations}
To run \vv{neo-2} with a Boozer file as input for the magnetic equilibrium,
the following input files are required:
\begin{itemize}
 \item \vv{neo.in} (see \ref{running_preparation_neoin})
 \item \vv{neo2.in} (see \ref{running_preparation_neo2in})
 \item Boozer file (*.bc) (see \ref{running_preparation_equilibrium})
 \item profile file (see \ref{running_preparation_profiles})
\end{itemize}
Additionaly, are some tools involved in running the code, that require
their own configuration files.

\subsubsection{neo.in\label{running_preparation_neoin}}
The first line of this file indicates the name of the Boozer file. Other
parameters are e.g. related to interpolation of the magnetic field
module and to the code NEO.

\subsubsection{neo2.in\label{running_preparation_neo2in}}
This file is a Fortran namelist file. It contains physical input
quantities and a lot of numerical and technical parameters. Here, the
most important (and new) parameters are described.
Check also \vv{neo2.in.par-full} in the DOC folder of the \vv{neo-2}
repository. The file should contain an up-to-date list of all the input
parameters together with default values and maybe a short explanation.

Note on naming convention: the prefix lsw and isw stand for logical
switch and integer switch, respectively.

\begin{itemize}
 \item \verb|boozer_s| \newline
 Defines the flux surface as normalized toroidal flux.
 \item \verb|conl_over_mfp|\newline
 This is the collisionality parameter. When provided positive it is
 $L_c/l_c$ (connection length over mean free path) and when provided as
 negative value it is $\kappa = 2/l_c$. Please be aware that in our
 papers we define $\kappa = 1/l_c$, while internally in \vv{neo-2} it has a
 slightly different normalization.
 \item \verb|mag_nperiod_min|\newline
 Only used for stellarators. Defines the minimum number of field periods
 until the field line is closed artificially.
\end{itemize}

New parameters related to the collision operator:
\begin{itemize}
 \item \verb|lag|\newline
 Number of basis functions
 \item \verb|leg|\newline
 Number of Legendre polynomials
 \item \verb|T_e|\newline
 Electron temperature for relativistic collision operator in eV.
 \item \verb|isw_relativistic|
 \begin{itemize}
  \item 0: Non-relativistic limit (Default)
  \item 1: Braams/Karney model. If $leg>1$, then higher Legendre
  polynomials are computed in the non-relativistic limit.
  \item 2: Direct evaluation of Beliaev/Budker form.
 \end{itemize}
 \item \verb|v_max_resolution|\newline
 Only affects level placement and defines the maximum normalized
 velocity that should be resolved by the grid. Experience showed that
 values of $2$ - $3$ are sufficient for reconstruction of the
 generalized Spitzer function up to $5$ times the thermal velocity.
 \item \verb|collop_base_prj|\newline
 Projection base for basis function expansion.
 \begin{itemize}
  \item 0: Generalized Laguerre polynomials of order $3/2$ (Default).
  \item 1: Standard polynomials $\phi_m(x) = x^m$.
  \item 2: Quadratic polynomials $\phi_m(x) = x^{2m}$.
  \item 10: Cubic Splines generated from a $y_m = (0, 0, ..., 1, ..., 0)$ grid.
  \item 11: General B-Splines (best choice).
 \end{itemize}

 \item \verb|collop_base_exp|\newline
 Expansion base for basis function expansion. See \verb|collop_base_prj|
 for parameters. At the moment it was only tested for
 \verb|collop_base_prj = collop_base_exp|.

 \item \verb|collop_bspline_order|\newline
 According to the B-Spline definition this is the order parameter $k$.
 As an example $k=3$ creates quadratic B-Splines and $k=4$ cubic
 B-Splines (best choice).

 \item \verb|collop_bspline_dist|\newline
 This parameter was introduces for testing a non-uniform knot
 distribution for B-Splines. Default value is $1$ which defines a
 uniform knot distribution (best choice).

 \item \verb|phi_x_max|\newline
 Important parameter for numerical integration and definition of
 B-Spline knot distribution. The B-Splines are distributed between
 $x=0$ and this value. A typical choice is $5$. Above this value the
 B-Splines are extrapolated with a Taylor series.

 \item \vv{mag_write_hdf5}\newline
 Creates \vv{magnetic.h5} which contains all information of the magnetic
 field as it is ``seen'' from \vv{neo-2}.

 \item \vv{lsw_save_dentf}, \vv{lsw_save_enetf}, \vv{lsw_save_spitf}\newline
 Defines if the gradient driven distribution function (mainly used for
 bootstrap studies) or the generalized Spitzer function is stored. These
 settings of course require \vv{neo-2} reconstruction runs.

 \item \vv{prop_reconstruct}\newline
 0 means a standard \vv{neo-2} run. If \vv{prop_write = 2}, then all
 information for subsequent reconstruction runs are stored. For full
 reconstruction \vv{neo-2} has to be run all reconstruction steps from 0 to
 3, where 3 is a service run which cleans up the directory and merges
 HDF5 files of this run. Note that for the parallelized stellarator version only
 reconstruction steps 0 and 2 can be parallelized with MPI.

\end{itemize}

% ----------------------------------------------------------------------
\subsection{Preparation of equilibrium\label{running_preparation_equilibrium}}
The main directory for everything related to ECCD is
\vv{/proj/plasma/Neo2/Interface/}. The Boozer files for ECCD runs are
located here: \vv{/proj/plasma/Neo2/Interface/Boozer/}.

Usually the W7-X files are already in the correct format to be read by
\vv{neo-2} despite of some additional comment lines at the beginning of the
file which can be safely removed. An example of the original file and
the slightly modified file (some comments have been removed) is given
here: \vv{/proj/plasma/Neo2/Interface/Boozer/w7x-m111-b3-i1/}.

% ----------------------------------------------------------------------
\subsection{Preparation of the plasma parameter profiles\label{running_preparation_profiles}}
The profiles for ECCD can be found here:
\vv{/proj/plasma/Neo2/Interface/Profiles/}. Profiles are usually
provided as text files, e.g.,
\vv{/proj/plasma/Neo2/Interface/Profiles/w7x-m111-b3-i1/prf.txt}. These
plasma parameter profiles are used for computation of the collisionality
parameter $\kappa$, which is then used as an input to \vv{neo-2}. Please note
that the collisionality is a function of the flux surface label. For the
converter is it necessary that the file has the appropriate format and
the that units are correct. A MATLAB script is given in the file
\vv{convert_prf.m}, while it should be noted that the reader section of
this script is adapted for one particular input file format. The output
file has to have the form of
\vv{/proj/plasma/Neo2/Interface/Profiles/w7x-m111-b3-i1/profiles.dat}.
\begin{itemize}
 \item 1. Column: Normalized toroidal flux (\vv{boozer_s}).
 \item 2. Column: Electron density in cm$^{-3}$.
 \item 3. Column: Electron temperature in eV.
 \item 4. Column: Effective charge $Z_\mathrm{eff}$.
\end{itemize}

% ----------------------------------------------------------------------
\subsection{Preparation of the run directories\label{running_preparation_directories}}
Next step is the preparation of the flux surface grid for the \vv{neo-2}
runs. It is started from an uniform grid where each grid point is
slightly shifted if a low-order rational flux surface is detected.
Afterwards the given profile file is read for computation of the
collisionality parameter (by spline interpolation along the radial
coordinate).

It is now necessary to copy the following required files to a new
empty directory,
\begin{verbatim}
boozer file
profile file
neo.in configuration file
create_surfaces.in configuration file
ln -s $NEO2PATH/tools/create_surfaces/Build/create_surfaces.x
\end{verbatim}
(You have to replace ``Build'' by the name of the build directory you
used.)

The namelist file \vv{create_surfaces.in} is described as follows:
\begin{itemize}
 \item \vv{s_beg} \newline
 First flux surface (normalized toroidal flux)
 \item \vv{s_end} \newline
 Last flux surface (normalized toroidal flux)
 \item \vv{s_steps} \newline
 Number of flux surfaces
 \item \vv{mag_nperiod_min} \newline
 Minimum number of toroidal field periods
 \item \vv{mag_nperiod_max} \newline
 Maximum number of toroidal field periods (required for avoiding
 low-order rational flux surfaces)
 \item \vv{output_file} \newline
 Output file with input parameters for \vv{neo-2}
 \item \vv{isw_create_surfaces} \newline
 Defines if flux surfaces should be found for \vv{neo-2} runs or if only the
 plasma parameter profiles should be splined (Default: true)
 \item \vv{profiles_file} \newline
 Input file for plasma parameter profiles (as it was generated in the
 last step)
\end{itemize}

Running \vv{./create_surfaces.x} creates a grid in \vv{boozer_s} that
fulfills the properties as defined in the configuration file. Afterwards a
spline interpolation is used to compute the collisionality parameter per
flux surface. The file \vv{surfaces.dat} (as defined by \vv{output_file}
in the configuration file) is structured as follows:
\begin{itemize}
 \item 1. Column: \vv{boozer_s}
 \item 2. Column: \vv{conl_over_mfp} (negative because this is $\kappa$
 as used in \vv{neo-2})
 \item 3. Column: \vv{Z_eff}
 \item 4. Column: \vv{T_e} (Electron temperature in eV)
\end{itemize}

Please note that the code for creating the surfaces allows for some
surfaces to have more periods than defined in \vv{mag_nperiod_max} in
order to not shift this surface to far away from the original
equidistant grid.

In the next step a template directory for the run directories has to be
created. Here, an example of \vv{neo2.in} is used and the executable of
\vv{neo-2} is copied from the project directory.
\begin{verbatim}
mkdir TEMPLATE_DIR
cd TEMPLATE_DIR
cp ../BOOZER_FILE_NAME.bc .
cp ../neo.in .
cp $NEO2PATH/NEO-2-PAR/Build/neo_2.x .
cp /proj/plasma/Neo2/Interface/Examples/neo2.in .
cp /proj/plasma/Neo2/Interface/Examples/condor-all.submit .
\end{verbatim}

The last line is only used if HTCondor should be used as batch system.
An alternative approach is the condor\_submit\_example file in the
\vv{DOC} folder of \vv{neo-2}.

In file \vv{neo2.in} the values of the four quantities
\vv{boozer_s} (in namelist settings), \vv{conl_over_mfp}, \vv{T_e} and
\vv{z_eff} (in namelist collision) have to be set to their name (with
case as given here) in \vv{< >} brackets. These serve as placeholders
that are replaced in the following step.

Next, the code
\vv{\$NEO2PATH/PythonScripts/create_surf.py} has to has to
be started, in the parent directory of the template directory.

This creates \vv{s_steps} directories, each with an modified
\vv{neo2.in} file. It is recommended to change into one of these
directories and to check if \vv{boozer_s}, \vv{conl_over_mfp},
\vv{z_eff} and \vv{T_e} have been set correctly. Note that the latter is
only used by \vv{neo-2} in the relativistic case, i.e. \vv{isw_relativistic > 1}.

The Python script has an output representing the directory names that
have been created. This output is required by some other tools, and thus
should be copied into a file named \vv{jobs_list.txt}. You can achieve
this by using either of the commands
\begin{verbatim}
./create_surf.py > jobs_list.txt
\end{verbatim}
or
\begin{verbatim}
./create_surf.py | tee jobs_list.txt
\end{verbatim}
instead of what is given above. We recomend the latter, as the output is
written to file and to the screen and thus allows you to check the
progress.

\subsubsection{Notes on resources}
The condor submit files determine the resources requested by the job in
the two fields \vv{request_cpus} (note that this number should match
the number for \vv{-np} in \vv{arguments}) and \vv{request_memory}.
The amount of memory required is fixed by the parameters of the jobs
(you may still request more to make sure they run on a specific subset
of nodes, but there are other options for this).
The number of processors can be choosen freely up to the number of
processors available per node, or even more, but this is untested and
may reduce performance due to limited bandwidth between nodes.

% ----------------------------------------------------------------------
\subsection{Running the jobs\label{running_running}}
In order to start the jobs the command
\begin{verbatim}
run_condor.sh condor-all.submit
\end{verbatim}
can be used (\vv{run_condor.sh} is part of \vv{neo-2}), which submits a
condor job for each directory given in
\vv{jobs_list.txt}. Checking the output of \vv{condor_q} shows the
number of submitted Condor jobs.

Another approach would be to use a condor submit file similar to
\vv{condor_submit_example} in the DOC folder of the \vv{neo-2} repository.
While the above will create one ``batch'' per subfolder, the
example file would create one ``batch'' in total, which has one job per
subfolder starting with \vv{es_}.
It can be used with
\begin{verbatim}
condor_submit condor_submit_example
\end{verbatim}

In either case (\vv{run_condor.sh} and \vv{condor_submit}), this runs
only \vv{prop_reconstruction = 0} (see entry in
sec.~\ref{running_preparation_neo2in}).
You have to rerun each folder also for 1,2 and 3 (3 can be replaced with
python3 function \vv{prop_reconstruct_3} of module \vv{hdf5tools}).
Changing the value can be done with the script \vv{switch_reconstruction.sh}
for a single file, or with python3 function
\begin{verbatim}
from scan_nonhdf5_tools import set_neo2in_reconstruction
set_neo2in_reconstruction(folder='./',
                          subfolder_pattern='s[123456789].*',
                          backup=False,
                          value=1)
\end{verbatim}
for all files.

Once the jobs have finished \vv{prop_reconstruction = 3}, the code
\begin{verbatim}
h5merge.x
\end{verbatim}
needs to be run. It can be found in \vv{libneo} in the tools subfolder.
This code merges the output file of \vv{neo-2} \vv{final.h5}
of each directory defined in \vv{jobs_list.txt} into one output file.
This output is file is already the one which is used by the \vv{neo-2}/TRAVIS
interface.

There is also a python3 function as alternative
\begin{verbatim}
import hdf5tools
hdf5tools.copy_hdf5_from_subfolders_to_single_file(path = './',
  infilename = 'final.h5',
  outfilename = 'collected_final.h5',
  only_base_path = True
  source_base_path = True)
\end{verbatim}
This function does not use file \vv{jobs_list.txt}, but instead checks
each folder for ``infilename'', and ignores folders that do not contain
a file of this name.

In either case the operation is probably limited by filesystem operations
and thus may take a while (in Graz is a gigabit network and speed is
about 30-40 MB/s, with respect to output file, i.e. an output file size
of 350 GB means this step will take approximately ten thousand seconds).

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Studies of the generalized Spitzer function\label{spitzer_function}}
The following works with the merged output of multiple flux surfaces or
with the output of a single flux surface.

The best option is the create a plot-directory inside of the run
directory of \vv{neo-2}:
\begin{verbatim}
mkdir PLOTS
cd PLOTS
ln -s /proj/plasma/Neo2/Interface/Spitzer_Interface/Build/neo2_g.x
cp /proj/plasma/Neo2/Interface/Examples/g_vs_lambda.in .
cp /proj/plasma/Neo2/Interface/Examples/spitzerinterface.in .
./neo2_g.x
\end{verbatim}

The config files are defined in the technical documentation of the
interface\footnote{\vv{/proj/plasma/Neo2/Interface/Documentation/Internal/Interface.pdf}}.
The result file (\vv{g_vs_lambda.h5}) can be plotted with MATLAB, e.g.,
with \vv{plot_g_xfix.m} from the main directory of all MATLAB scripts
related to ECCD:
\begin{verbatim}
/proj/plasma/Neo2/Interface/Matlab
\end{verbatim}

Please note that the information about the magnetic field is stored per
\vv{neo-2} run in \vv{magnetics.h5} (only NEO-2-PAR has support for this).
This file can also be used to plot the field module along the field line
for visualization of the observations points. A useful MATLAB script is
\vv{plot_levels_pgf.m} which also exports the correct format for
\vv{g_vs_lambda.in}.

Further plots of the magnetic field, such as a 2D distribution of the
magnetic field module on a particular flux surface can be made with the
tools located here:
\begin{verbatim}
 /proj/plasma/Neo2/Interface/Magfie
\end{verbatim}

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Running TRAVIS\label{travis}}
TRAVIS is located here:
\begin{verbatim}
/proj/plasma/TRAVIS
\end{verbatim}
In order to start the GUI the following steps are necessary:
\begin{itemize}
 \item \vv{/proj/plasma/TRAVIS/TRAVIS-16-04-08/bin/bin_ECRH/travisGUI64_IOTA}
 \item Say yes to the question if the last project should be loaded.
 \item Preferences -> Advanced -> Expert mode
 \item On the top right corner select the TRAVIS kernel for your demands
 (SYNCH, \vv{neo-2} non-rel, \vv{neo-2} rel, pure TRAVIS)
\end{itemize}

The configuration with HDF5 input file for the interface is used has to
be set in the usual \vv{spitzerinterface.in} file which is located in
the RUN/TEMP directory of TRAVIS. This can also be selected in the
settings of TRAVIS. If there is an upgrade to the \vv{neo-2} interface or to
SYNCH, the TRAVIS kernels have to be recompiled. This has to be done
here:
\begin{verbatim}
/proj/plasma/TRAVIS/TRAVIS-16-04-08/TRAVIS-src
\end{verbatim}
with the command \vv{make gnu}. However, before several config files
have to be set, especially, \vv{makeunix/mkfiles/defs.mk}.

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Bootstrap\label{bootstrap}}
For plotting the bootstrap coefficient it is not necessary to run \vv{neo-2}
in the reconstruction mode. Plotting can be done with the script
\vv{plot_evolve_h5.m}.
This script has to be started in the directory where the complete \vv{neo-2}
run is located.

For investigating the gradient driven distribution it is necessary to
complete a full reconstruction run of \vv{neo-2} (steps 0 to 3). Then one can
use the MATLAB scripts from
\begin{verbatim}
/proj/plasma/DOCUMENTS/PHD_Gernot/2017_BOOTSTRAP
\end{verbatim}
in order to evaluate the antisymmetric part of the gradient driven
distribution function at various observation points. The script
\vv{count_maxima_bootstrap.m} is a good place to start with.

This script reads the file \vv{magnetics.h5} from the run directory of
\vv{neo-2} and shows a part of the field line. The variable \vv{point_phimfl}
defines the angle along the field line defining the observation point
indicated as dot in the figure. The MATLAB script has an output on the
console of the following format:
\begin{verbatim}
                  s              theta                phi     tag
     2.50000000e-01     4.43238764e+00     5.98323740e+00      p1
\end{verbatim}

This is the input for the \vv{neo-2} interface computing the distribution
function at the defined observation point(s).

A good start for beginning is with LHD runs for the bootstrap tasks, e.g.:
\begin{verbatim}
/temp/gernot_k/Neo2/RunsByDate/2017_07_Bootstrap/lhd/
Short/conl_over_mfp=5m5-boozer_s=0.25d0-mag_nperiod_min=1-bsfunc_local_err=1m1
\end{verbatim}
Note that a newline was added in the above path, to improve readability.
In the subdirectory \vv{PLOTS} a symbolic link to the distribution
function plotter for bootstrap is already prepared
(\vv{/proj/plasma/Neo2/Interface/Spitzer_Interface/Build/dentf_lorentz.x}).
Please note that this executable is slightly different to \vv{neo2_g.x}
as used for Spitzer function, because a significant higher number of
$x$-values (velocity module) are used for plotting. In
\vv{g_vs_lambda.in} the two lines from the output of the MATLAB script
should be inserted. Next, \vv{./dentf_lorentz.x} can be run in the
\vv{../PLOTS} directory. This produces \vv{g_vs_lambda.h5} (as before
for ECCD tasks). Finally, the MATLAB script from before can be run a
second time and the distribution function including the relevant maxima
is plotted.

Please note, that if the observation point is changed,
\vv{./dentf_lorentz.x} has to be run again with an updated input file.
Otherwise, the MATALB plotter will show data from the previous run of
the interface.

The settings in \vv{spitzerinterface.in} are described in the interface
documentation. It should be noted that the parameter \vv{plot_source}
can have the values \vv{dentf}, \vv{enetf}, and \vv{spitf} defining the
distribution function to be plotted.

% ----------------------------------------------------------------------
\subsection{Scripts for running the jobs in Condor}
In the \vv{ShellScripts} subfolder of \vv{neo-2}
are two important scripts which are useful for running many
instances of \vv{neo-2} at the same time via Condor.

The script \vv{create_dirs_paramvalue.sh} reads a file called
\vv{dirs_list.txt} and copies from a directory \vv{TEMPLATE_DIR} all
files to given subdirectories and modifies each \vv{neo2.in}. It is best
explained by looking at some examples, e.g., here:
\begin{verbatim}
/temp/gernot_k/Neo2/RunsByDate/2017_07_Bootstrap/ratfieldline
\end{verbatim}

For running the jobs the same script can be used as introduced in the
ECCD section of this writeup (\vv{run_condor.sh}). Here, the content of
the condor submission file is described in more detail:
\begin{verbatim}
Executable = NEO2-all.sh
Universe   = vanilla

Error      = NEO2-0.e$(Cluster)
Log        = NEO2-0.l$(Cluster)
Output     = NEO2-0.o$(Cluster)

notification = Never

request_cpus   = 4
request_memory = 30 * 1024

Getenv     = true
should_transfer_files = NEVER

run_as_owner = true
#requirements = (TARGET.Machine != "faepop02") && (TARGET.Machine != "faepop03")
#environment = "XDG_CACHE_HOME=/tmp"

Queue
\end{verbatim}
The first line defines the executable which has to be located in
\vv{/temp/} because Condor does not have access to AFS. The Universe
defines the Condor environment and is always vanilla at our institute.
The next three lines define the file names for the output files, where
(\$Cluster) will be an ID which is created by Condor defining the job.
The notification line defines if a mail is sent when the job has
finished.

The launch script \vv{NEO2-all.sh} (located in the template directories)
is written in a way that MPI is used for running \vv{neo-2}. Here, the number
of processors used is given by \vv{request_cpus} in the Condor
submission file. In the vanilla universe it is not possible to run the
MPI jobs across machines so that a typical number is 4 to 8 at our
institute. The requested memory (30*1024 MB = 30 GB) defines how much
memory the job will be allowed to use (all MPI processes together).
If Condor jobs are failing then one should look into the
file NEO-2.e(\$SomeNumber) because there the error messages are located.
Some IEEE underflow errors are "normal" at the moment. They arise from
some underflow during the integration of the matrix elements and might
be fixed in the future.

The requirements and ernvironment lines are commented, i.g. not actually
considered in this example.
The environment needs to be set on the machines of the Institute of
Theoretical and Computational Physics at TU Graz.
The requirements do not have to be changed except one would like
to exclude a special machine from Condor.
If your jobs fits into the memory of regular machines, then you should
exclude the high memory machines (e.g. faepop12 and faepop13), so as to
not block them for people who need the memory.

% ----------------------------------------------------------------------
\subsection{Boozer file for rational field line}
For creating a magnetic equilibrium with toroidal perturbations, the
MATLAB script
\vv{boozer_rational.m}
can be used.

This script starts from \vv{tok-synch2.bc}, adds a perturbation field
with given mode number and perturbation amplitude. In addition, a flat
rotational transform profile can be created.

Running the script \vv{plot_evolve_h5.m} in the parent directory
containing the run directories, plots the bootstrap coefficient. Using
\vv{plot_levels.m} in a
specific run directory (containing \vv{magnetics.h5}) plots the magnetic
field module along the field line with level distribution.

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Running NEO-2 on marconi\label{marconi}}
While in general the steps to run \vv{neo-2} are the same, there might
be some points that differ. Those are covered here for running the relativistic version of \vv{neo-2}
on the marconi cluster, as well as some additional hints.

Be sure to also take a look at the documentation
\begin{verbatim}
https://wiki.u-gov.it/confluence/display/SCAIUS/HPC+User+Guide
\end{verbatim}

% ----------------------------------------------------------------------
\subsection{Getting data to/from marconi}
Note that at the time of writing using \vv{scp}/\vv{rsync} on marconi to
Graz does not work, i.e. you have to run these commands in Graz.
You can check if you might suffer from the same problem by using \vv{ping}:
\begin{verbatim}
ping localadress
\end{verbatim}
if the packages arrive, then \vv{scp}/\vv{rsync} work.

% ----------------------------------------------------------------------
\subsection{Preparations}
First you will need a user account for cineca, then acess to the HPC
system. The necessary steps will not be repeated here, check the
documentation for this (see link above).

As other HPC environments marconi uses a module system, to provide
different versions of software (e.g. compiler).

The following list of modules was used, and loaded in \vv{.bashrc}

\begin{verbatim}
module load cmake/3.18.2
module load intel/pe-xe-2018--binary
module load intelmpi/2018--binary
module load blas/3.8.0--intel--pe-xe-2018--binary
module load lapack/3.8.0--intel--pe-xe-2018--binary
module load metis/5.1.0--intel--pe-xe-2018--binary
module load mkl/2018--binary

# hdf5 requirements
module load szip/2.1--gnu--6.1.0
module load zlib/1.2.8--gnu--6.1.0

module load hdf5/1.10.4--intelmpi--2018--binary
module load python/3.6.4
# following two are scipy requirements, scipy required for testing arnoldi
module load mkl/2018--binary
numpy/1.14.0--python--3.6.4
module load scipy/1.2.2--python--3.6.4
\end{verbatim}

Note that there was a newer version of the intel compiler available,
but some of the required modules had the 2018 compiler as prerequisite.

Additionally these libraries had to be compiled:
\begin{itemize}
  \item gsl
  \item fgsl
  \item SuiteSparse
\end{itemize}

Note that there is a gsl module available, but for unknown reasons it
was not found during configuration.

Matching releases of \vv{gsl} and \vv{fgsl} have been downloaded localy
and then copied to marconi.

For the libraries shell scripts \vv{buildcommands.sh} have been written.
Those of \vv{gsl} is
\begin{verbatim}
#! /bin/bash

./configure --prefix=${HOME}/local/
make
make install
\end{verbatim}
Those of \vv{fgsl}
\begin{verbatim}
#! /bin/bash

export gsl_LIBS=`gsl-config --libs`
export gsl_CFLAGS=`gsl-config --cflags`
./configure --prefix=${HOME}/local/
make
make install
\end{verbatim}
but before this can be used, once the commands
\begin{verbatim}
mkdir m4
autoreconf -i
\end{verbatim}
are required.

SuiteSparse was cloned from its git repository and then compiled
\begin{verbatim}
git clone https://github.com/DrTimothyAldenDavis/SuiteSparse.git
make BLAS="-L${MKLROOT}/lib/intel64/ -lmkl_rt"
\end{verbatim}
Note that not all parts could be comiled succesfully, but as the required
parts could be build, this was not further investigated. (\vv{SPQR}
did not work because of compiler version.)

To make sure \vv{gsl} and \vv{fgsl} are found, the following lines
where added to the \vv{.bashrc}
\begin{verbatim}
export PATH=${HOME}/local/bin/:${PATH}
export LD_LIBRARY_PATH=${HOME}/local/lib/:${LD_LIBRARY_PATH}
export PKG_CONFIG_PATH=${HOME}/local/lib/pkgconfig/:${PKG_CONFIG_PATH}
\end{verbatim}

\subsection{Compiling libneo and NEO-2}
Some specific modifications had to be made to get \vv{libneo} and
\vv{neo-2} to compile on marconi.

For \vv{libneo} the line
\begin{verbatim}
target_link_libraries(libneo lapack)
\end{verbatim}
in \vv{CMakeLists.in} had to be commented.
Additionally the bodies of the routines \vv{h5_add_complex_1} and
\vv{h5_add_complex_2} in \vv{src/hdf5_tools/hdf5_tools.f90} had to be
commented. This is due to a bug in the intel compiler, which does not
allow to use \vv{complex_number\%re} or \vv{complex_number\%im} (The
slash \vv{\\} had to be added due to \LaTeX) as
parameter for intent out or inout arguments. It is expected that this
bug is also in the 2020 version, while newer versions might have it
fixed.
This should not be a problem for \vv{neo-2} as so far only real values
need to be read.

For \vv{neo-2}, in \vv{ProjectConfig.cmake.in} the lines
\begin{verbatim}
set(SUITESPARSE_INCLUDE_DIR_HINTS
    /marconi/home/userexternal/rbuchhol/Programs/SuiteSparse/include/)
set(SUITESPARSE_LIBRARY_DIR_HINTS
    /marconi/home/userexternal/rbuchhol/Programs/SuiteSparse/lib/)
\end{verbatim}
have been added to make sure the \vv{SuiteSparse} library is found.
The setting of PROJECTLIBS and MPE\_PATH have been commented, while
setting of LIBNEOLIBS has been changed to
\begin{verbatim}
set(LIBNEOLIBS /marconi/home/userexternal/rbuchhol/Programs/libneo/
    CACHE STRING "libneo library path")
\end{verbatim}
Final change for was in the else-clause of the compiler-id check. Where
the following two lines belong
\begin{verbatim}
set(FGSL_PATH /marconi/home/userexternal/rbuchhol/local/)
set(NEO2_Libs ${LIBNEOLIBS}/Build-Intel/ CACHE STRING "Libneo path")
\end{verbatim}

In \vv{CMakeSources.in} the lines corresponding to SuperLU have been
commented, as it was decided to compile only one of SuiteSparse and SuperLU.

In \vv{CMakeLists.txt} lines
\begin{verbatim}
find_package(BLAS REQUIRED)
find_package(LAPACK REQUIRED)
\end{verbatim}
(after corresponding call for MPI) and
\begin{verbatim}
include_directories(${FGSL_INC})
include_directories(${HDF5_Fortran_INCLUDE_DIRS})
\end{verbatim}
have been added (former with other include\_directory calls, the latter
after the find\_package call).
The lines correspondig to SuperLU have been commented.
Searching for gsl is changed to (from find\_library)
\begin{verbatim}
find_package(GSL COMPONENTS GSL_LIBRARY GSL_CBLAS_LIBRARY REQUIRED)
\end{verbatim}
Finally in \vv{target_link_libraries} the entries for \vv{gsl}, \vv{fgsl} and
\vv{blas}/\vv{lapack} are changed
\begin{verbatim}
${fgsl_lib}
${gsl_lib}
${GSL_LIBRARIES}
${BLAS_LIBRARIES}
${LAPACK_LIBRARIES}
\end{verbatim}

In \vv{COMMON/sparse_mod.f90} the body of the \vv{SuperLU} subroutines
have been commented.
Finally line 472 (at the time of writing) of \vv{COMMON/binarysplit_mod.f90}
has to be commented (a call of \vv{h5_add} inside of a if(false) block).

\subsubsection{Additional hints}
If you get an error or the form
\begin{verbatim}
/PATH/file.f90(line): error #7002: Error in opening the compiled module file. &
  &  Check INCLUDE paths. [HDF5]
  USE hdf5_tools
\end{verbatim}
(same has been seen with \vv{[GSL]} and \vv{use fgsl}), then the problem
is not that \vv{hdf5_tools} (or the modules) is not found, the problem
is that \vv{hdf5} (or the modules) is not found.
Note that a line break has been added, denoted with \& at the end of the
line and the beginning of the continuation line.

Error messages of the kind
\begin{verbatim}
PATH/sparse_mod.f90.o: In function `sparse_mod_mp_sparse_solve_superlucomplex_b1_':
sparse_mod.f90:(.text+0xbc5a): undefined reference to `c_fortran_zgssv_'
sparse_mod.f90:(.text+0xc540): undefined reference to `c_fortran_zgssv_'
sparse_mod.f90:(.text+0xcdcf): undefined reference to `c_fortran_zgssv_'
\end{verbatim}
mean that there are still references to \vv{SuperLU} routines, either
comment them or also compile SuperLU.

\clearpage

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{List of scripts/codes used besides neo-2}
Here four tables list executables, shell scripts, python scripts and octave/matlab scripts,
respectively, that are mentioned in this document besides \vv{neo-2}.

Note that \vv{$INTERFACE} is here short for \vv{/proj/plasma/Neo2/Interface/}.

\begin{table}[h]
\centering
\begin{tabular}{l|l}
Name/path   & Comment \\ \hline
tools/create\_surfaces & convert profile to form used by create\_surf.py \\
libneo:tools/h5merge & merges final.h5 of subdirectories \\
\$INTERFACE/Spitzer\_Interface/Build/neo2\_g.x & \\
\$INTERFACE/Spitzer\_Interface/Build/dentf\_lorentz.x &
\end{tabular}
\caption{additional executables used besides neo-2}
\label{tab:additionalcodes}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l|l}
Name/path   & Comment \\ \hline
run\_condor.sh & \\
create\_dirs\_paramvalue.sh & \\
?/NEO2-all.sh & wrapper for running with condor
\end{tabular}
\caption{additional shell scripts used}
\label{tab:additionalshellscripts}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l|l}
Name/path   & Comment \\ \hline
PythonScripts/create\_surf.py & creates directories from template
\end{tabular}
\caption{additional python3 scripts used}
\label{tab:additionalpython3scripts}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l|l}
Name/path   & Comment \\ \hline
boozer\_rational.m & \\
convert\_prf.m & convert profile to form used by create\_surfaces \\
count\_maxima\_bootstrap.m & \\
plot\_evolve\_h5.m & \\
plot\_g\_lamfix.m & plot distribtion function, velocity module fixed \\
plot\_g\_xfix.m & plot distribtion function, pitch angle fixed \\
plot\_levels.m & requires \vv{magnetics.h5}) \\
plot\_levels\_pgf.m &
\end{tabular}
\caption{additional octave scripts used}
\label{tab:additionaloctavescripts}
\end{table}

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Open tasks}
\begin{itemize}
 \item Include NBI source term.
 \item Distribution function plotter in NEO-2-QL.
 \item Cleanup CMake files and prepare for next major Debian upgrade
 (maybe works already).
 \item Save memory in level placement. (Done, December 2017)
 \item Diffusion coefficients with relativistic collision model in
 NEO-2-PAR. (Done, May 2018)
 \item Adaptive Runge-Kutta solver for EFIT runs. (Done, May 2018)
 \item Different integration accuracy settings needed for single species
 and multispecies run. This needs to be a configuration for the input
 file. (Done, May 2018)
 \item Create magnetic equilibrium where Shaing-Callen limit can be
 reached with \vv{neo-2}. (Done, June 2018)
\end{itemize}

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{More useful scripts}
\begin{itemize}
 \item \vv{hdf5struct.m} Converts HDF5 file to MATLAB structure. Note,
 that this does not work with octave, but is not required, as \vv{load}
 in octave loads a hdf5 file and converts it to a structure.
 \item \vv{plot_g_xfix.m} Plots the distribution function for fixed
 velocity module.
 \item \vv{plot_g_lamfix.m} Plots the distribution function for fixed
 pitch angle.
 \item \vv{plot_levels.m} Plots the field module along the field line
 with level distribution (needs \vv{magnetics.h5}).
\end{itemize}

%\clearpage
%\renewcommand{\bibname}{References}
%\printbibliography[notcategory=fullcited]

\end{document}
