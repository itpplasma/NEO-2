\documentclass{article}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amsmath}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[a4paper, top=3cm, bottom=3cm]{geometry}
\usepackage{authblk}
\usepackage{setspace}
\usepackage{csquotes}
\usepackage{booktabs}
%%%%%%%%%%%%%%%%%% My preamble %%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[numbers, sort&compress]{natbib}
%\setcitestyle{numbers,square,comma,aysep={,},yysep={,},notesep={,}}
%\bibliographystyle{unsrtnat}
%\bibliographystyle{IEEEtranSN}

%References
\usepackage[
    bibstyle=phys,
    biblabel=brackets,
    citestyle=numeric-comp,
    isbn=false,
    doi=false,
    sorting=none,
    url=false,
    defernumbers=true,
    bibencoding=utf8,
    backend=biber,
    %maxbibnames=3,maxcitenames=3,
    %minnames=3,
    maxnames=30,
    ]{biblatex}
\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}
\addbibresource[]{eccd_ref.bib}
\DeclareBibliographyCategory{fullcited}
\newcommand{\mybibexclude}[1]{\addtocategory{fullcited}{#1}}
\DeclareFieldFormat{titlecase}{\MakeCapital{#1}}
\DeclareFieldFormat{sentencecase}{\MakeSentenceCase{#1}}

\usepackage[font=small,labelfont=bf]{caption}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{listings}

\usepackage{hyperref}

\newcommand{\be}[1]{\begin{equation} \label{#1}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}[1]{\begin{eqnarray} \label{#1}}
\newcommand{\eea}{\end{eqnarray}}
\setlength\parindent{0pt}

\newcommand{\br}{{\bf r}}
\newcommand{\bh}{{\bf h}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bB}{{\bf B}}
\newcommand{\rd}{{\rm d}}
\newcommand{\eq}[1]{(\ref{#1})}
\newcommand{\bp}{{\bf p}}
\newcommand{\difp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iotabar}{\mbox{$\iota$\hspace{-0.365em}-}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\lstset{basicstyle=\ttfamily}
\newcommand{\vb}{\lstinline}
\newcommand{\vv}[1]{\texttt{\detokenize{#1}}}

\title{\textbf{Technical Documentation of NEO-2\\November 2021}}

\author[1]{Rico~Buchholz}
\author[1]{Winfried~Kernbichler}
\author[1]{Sergei~Kasilov}
\affil[1]{Fusion@\"OAW, Institute of Theoretical and Computational Physics, Graz University of Technology, Petersgasse 16, 8010 Graz, Austria}
\renewcommand\Affilfont{\itshape\small}

\date{}

\begin{document}
\onehalfspacing

\maketitle

\section{Introduction}
This serves as documentation on how to calculate and store the
generalized Spitzer function (electron cyclotron current drive (ECCD)
efficiency) or evaluate transport coefficients with \vv{NEO-2} and how
to do ECCD simulations with TRAVIS using \vv{NEO-2} input of ECCD efficiency.
As such it describes the required tasks, but not the
physical background.
The relevant physical background can be found in
\begin{itemize}
  \item W. Kernbichler, S. Kasilov, G. Kapper, A. F. Martitsch, V. Nyemov, C. G. Albert, \& M. F. Heyn (2016). Solution of drift kinetic equation in stellarators and tokamaks with broken symmetry using the code NEO-2. Plasma Physics and Controlled Fusion, 58, 104001. \url{https://doi.org/10.1088/0741-3335/58/10/104001}

  \item G. Kapper, S. Kasilov, W. Kernbichler, A. F. Martitsch, M. F. Heyn, N. B. Marushchenko, \& Y. Turkin (2016). Electron cyclotron current drive simulations for finite collisionality plasmas in Wendelstein 7-X using the full linearized collision model. Physics of plasmas, 23(11), 112511. \url{https://doi.org/10.1063/1.4968234}

  \item G. Kapper, S. Kasilov, W. Kernbichler, \& M. Aradi (2018). Evaluation of relativistic transport coefficients and the generalized Spitzer function in devices with 3D geometry and finite collisionality. Physics of plasmas, 25(12), 122509. \url{https://doi.org/10.1063/1.5063564}
\end{itemize}

Note that the variable \vv{NEO2DATAPATH} used throughout this document,
refers to \vv{/proj/plasma/Neo2/} at the ITCP of TU Graz.

In case of questions regarding this document or \vv{NEO-2}, the authors
can be reached by e-mail\footnote{\makeatletter \url{buchholz@tugraz.at}
and \url{winfried.kernbichler@tugraz.at} \makeatother}.

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Short summary}
Below, the required input files and the sequence of all necessary
actions for computation and storing the generalized Spitzer function is
summarized. In particular, this is a reminder for  those already
familiar with \vv{NEO-2}. Other usage of \vv{NEO-2} is not described in this
summary but is described in more details in the main text.
\subsection{Required input}
\begin{itemize}
  \item boozer file
  \item profile file (density, temperature, effective charge)
  \item neo.in
  \item create\_surfaces.in
  \item neo2.in
  \item (condor submit file|slurm submit file)
\end{itemize}

\subsection{Required steps}
\begin{itemize}
  \item get profiles (Sec.~\ref{running_preparation_profiles})
  \item get magnetic field as boozer file (Sec.~\ref{running_preparation_equilibrium})
  \item make sure you have all required libraries (Sec.~\ref{technicalbackground_libraries})
  \item clone/update \vv{libneo} and \vv{NEO-2} repositories (Sec.~\ref{technicalbackground_git})
  \item compile the codes (Sec.~\ref{technicalbackground_cmake})
  \item create working directory, and a template directory in this (Sec.~\ref{running_preparation_directories})
  \item transform input into required form for create\_surfaces.x (Sec.~\ref{running_preparation_profiles})
  \item get/create configuration files (Sec.~\ref{running_preparation_neo2in})
  \item check/edit settings as necessary (Sec.~\ref{running_preparation_neo2in})
  \item run create\_surfaces.x (Sec.~\ref{running_preparation_directories})
  \item run create\_surf.py (Sec.~\ref{running_preparation_directories})
  \item run neo2.x for prop\_reconstruct=0 (Sec.~\ref{running_running})
  \item run neo2.x for prop\_reconstruct=1 (Sec.~\ref{running_running})
  \item run neo2.x for prop\_reconstruct=2 (Sec.~\ref{running_running})
  \item run neo2.x for prop\_reconstruct=3 (Sec.~\ref{running_running})
  \item run h5merge.x to collect data (Sec.~\ref{running_running})
  \item checking output (e.g. plotting) (Sec.~\ref{spitzer_function}, \ref{travis} and \ref{bootstrap})
\end{itemize}

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Technical background on NEO-2}
\subsection{Overview}
In the last years the development of \vv{NEO-2} has split into two branches,
namely the more general branch for stellarators (and axisymmetric
tokamaks) and the quasilinear version for tokamaks with 3D magnetic
perturbations. The main differences between these two versions at time
of writing this document is given in the following Table~\ref{tab:neo2branches}.

\begin{table}[h]
\centering
\begin{tabular}{lll}
Internal name   & NEO-2-PAR & NEO-2-QL\\
Equilibrium     & Tokamak/Stellarator & Non-axisymmetric tokamak\\
Multiple ion species & No & Yes\\
Relativistic    & Yes & Yes (but not for multispecies)\\
Parallelization & Yes (field line) & Yes (Species)\\
Solver          & $1^\mathrm{st}$ Order & $1^\mathrm{st}$ and $2^\mathrm{st}$ Order%\\
%Distribution function & Yes & Partial\footnote{For plotting only, no interface for fast interpolation}
\end{tabular}
\caption{Comparison of \vv{NEO-2} branches}
\label{tab:neo2branches}
\end{table}

% ----------------------------------------------------------------------
\subsection{Libraries\label{technicalbackground_libraries}}
\vv{NEO-2} depends on a number of external libraries. These libraries
are:
\begin{itemize}
\item SuiteSparse
\item Metis
\item SuperLU
\item GSL
\item FGSL
\item HDF5 - interfaced via hdf5tools
\item OpenMPI - interfaced via MyMPILib
\item libneo
\end{itemize}

SuiteSparse is used for solving the sparse linear system of differential
equations in the ripple solver. Metis is used by SuiteSparse for speedup
of some specific routines and is not obligatory. SuperLU can be used
instead of SuiteSparse by a switch in the input file.

The GSL (Gnu Scientific Library) and its Fortran Interface FGSL are used
for several purposes. The first one provides very efficient numerical
integration methods used for the computation of the matrix elements of
the collision operator in \verb|collop_compute.f90|. Additionally, GSL
provides B-spline routines, also used for the collision operator module
(basis functions). While \vv{gsl} usally installed on (scientific) linux
systems, \vv{fgsl} is not. You thus probably need to compile it on your
own. Be sure to select an \vv{fgsl} version that is suitable for the
\vv{gsl} version used.

The HDF5 library is required for modern I/O of \vv{NEO-2}. One advantage over
text files is that each variable (dataset) has a unique name and can
store additional attributes such as units and/or comments. The usage of HDF5
was unavoidable for storing the large datasets that occur when computing
the generalized Spitzer function for stellarators ($\gtrsim 10\mathrm{GB}$ with \vv{hdf5}
for a single flux surface). In order to simplify
the calls a collection of wrapper routines has been created. This
project has grown over the time so that it was decided to use these
wrapper functions also in other projects, as in the interface, so that
it became an own library called \vv{hdf5tools}.

For parallelization the Message Passing Interface (MPI) is used. The
usage of MPI in \vv{NEO-2} is performed via an own library, called MyMPILib,
and is not restricted to a particular MPI implementation. At our
institute it is linked against OpenMPI, where on clusters Intel MPI is
mostly used. MyMPILib was developed so that in the code of \vv{NEO-2} no
native MPI commands have to be used.

Note that it was decided to no longer have \vv{hdf5tools} and
\vv{MyMPILib} as separate libraries, but to add them to \vv{libneo},
which is intended to collect code that is used by multiple programs of
the group. You can get \vv{libneo} from github (see
Sec.~\ref{technicalbackground_git}).

% ----------------------------------------------------------------------
\subsection{The Git repository\label{technicalbackground_git}}
Since 2013 the general version of \vv{NEO-2} is under Git version control.
Later, also the quasilinear version was added to this repository
including its code history. The official repository is located on github
at \url{https://github.com/itpplasma/NEO-2/}.
At the moment it is set to private, which means you need to be logged in
to be able to see it.

In order to start a new local working copy, it is required to do
\begin{verbatim}
git clone git@github.com:itpplasma/NEO-2.git .
\end{verbatim}
in an empty directory (do not oversee the point at the end of the
command line which defines the current directory).

The same holds for \vv{libneo}, \url{https://github.com/itpplasma/libneo/},
which is required by \vv{NEO-2}. Here the command is
\begin{verbatim}
git clone git@github.com:itpplasma/libneo.git .
\end{verbatim}
which should be done in another empty directory.

Once you have cloned \vv{libneo} and \vv{NEO-2} you should set the
variables \vv{LIBNEOPATH} and \vv{NEO2PATH} in your \vv{.bashrc} file.
In case you used above commands in your home folder the required lines
could read
\begin{verbatim}
export NEO2PATH=${HOME}/NEO-2/
export LIBNEOPATH=${HOME}/libneo/
\end{verbatim}
Variable \vv{NEO2PATH} is also used throughout this document.

Three other variables that you might want to adapted are \vv{PATH},
\vv{PYTHONPATH} and \vv{MATLABPATH}. Adding the script paths of \vv{NEO-2}
and \vv{libneo} to \vv{PATH} allows you to use the scripts within without
having to type the full path or to copy/link them.
The other two variables make sure python/matlab, respectively will find
the corresponding scripts.
For octave you can add a line
\begin{verbatim}
addpath('/path/to/NEO-2/OctaveScripts/', '-begin')
\end{verbatim}
in file \vv{.octaverc} (not sure if environment variables work in this
file, i.e. if \vv{NEO2PATH} can be used).

% ----------------------------------------------------------------------
\subsection{CMake and Compiling \label{technicalbackground_cmake}}
The build system of \vv{NEO-2} and \vv{libneo} is CMake.
The main advantage is that this build system takes care of correct
compilation order of the source files (dependency resolving) and that
routines can be defined to automatically detect the installed
libraries. For a range of often used libraries there are already routines
available. Using
CMake the source- and build-directories can be separated from each
other, and both codes require that this is done.

The configuration file for \vv{cmake} is called \verb|CMakeLists.txt|.
Thus, in order to build
the source code it is necessary to create a build-directory. For reasons
of readability it was decided to put the source files to be compiled in
a separate file, called \vv{CMakeSources.in}. Additionaly, some of the
settings for paths are in a separate file, \vv{ProjectConfig.cmake.in}.
Mainly these settings are related to required libraries (see
sec.~\ref{technicalbackground_libraries}).
These files are then included in \vv{CMakeLists.txt}.

For testing purposes a build-directory is now created for \vv{libneo}:
\begin{verbatim}
mkdir Build-Test
cd Build-Test
cmake ..
make
\end{verbatim}

Running the command \verb|cmake ..| is only necessary the first time a
new build directory is used. This command needs to know where
\verb|CMakeLists.txt| is located (therefore the \verb|..|). A lot
happens when this command is called, the most important steps are the
Fortran compiler and library detection. A standard Makefile is generated
which can be used by running \vv{make}.

Before going on to compile \vv{NEO-2} you should also compile the tool
\vv{h5merge} in the corresponding subfolder. The procedure to do so is
the same, just in the subfolder \vv{tools/h5merge}.

After libneo is compiled, you can compile \vv{NEO-2}:
\begin{verbatim}
cd NEO-2-PAR
mkdir Build-Test
cd Build-Test
cmake ..
make
\end{verbatim}

The first step is required as the two versions of \vv{NEO-2} have
separate folders and separate build-configuration files.

Also in this case there is a tool that should be compiled,
\vv{create_surfaces}.

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Running an ECCD precomputation}
\subsection{Preparations}
To run \vv{NEO-2} with a Boozer file as input for the magnetic equilibrium,
the following input files are required:
\begin{itemize}
 \item \vv{neo.in} (see \ref{running_preparation_neoin})
 \item \vv{neo2.in} (see \ref{running_preparation_neo2in})
 \item Boozer file (*.bc) (see \ref{running_preparation_equilibrium})
 \item profile file (see \ref{running_preparation_profiles})
\end{itemize}
Additionaly, are some tools involved in running the code, that require
their own configuration files.

\subsubsection{neo.in\label{running_preparation_neoin}}
The first line of this file indicates the name of the Boozer file. Other
parameters are e.g. related to interpolation of the magnetic field
module and to the code NEO.

\subsubsection{neo2.in\label{running_preparation_neo2in}}
This file is a Fortran namelist file. It contains physical input
quantities, as well as numerical and technical parameters. Here, the
most important parameters are described.
Check also \vv{neo2.in.par-full} in the DOC folder of the \vv{NEO-2}
repository. The file should contain an up-to-date list of all the input
parameters together with default values and maybe a short explanation.

Note on naming convention: the prefix lsw and isw stand for logical
switch and integer switch, respectively.

\begin{itemize}
 \item \verb|boozer_s| \newline
 Defines the flux surface as normalized toroidal flux.
 \item \verb|conl_over_mfp|\newline
 This is the collisionality parameter. When provided positive it is
 $L_c/l_c$ (connection length over mean free path) and when provided as
 negative value it is $\kappa = 2/l_c$. Please be aware that in our
 papers we define $\kappa = 1/l_c$, while internally in \vv{NEO-2} it has a
 slightly different normalization.
 \item \verb|mag_nperiod_min|\newline
 Only used for stellarators. Defines the minimum number of field periods
 until the field line is closed artificially.
\end{itemize}

New parameters related to the collision operator:
\begin{itemize}
 \item \verb|lag|\newline
 Number of basis functions
 \item \verb|leg|\newline
 Number of Legendre polynomials
 \item \verb|T_e|\newline
 Electron temperature for relativistic collision operator in eV.
 \item \verb|isw_relativistic|
 \begin{itemize}
  \item 0: Non-relativistic limit (Default)
  \item 1: Braams/Karney model. If $leg>1$, then higher Legendre
  polynomials are computed in the non-relativistic limit.
  \item 2: Direct evaluation of Beliaev/Budker form.
 \end{itemize}
 \item \verb|v_max_resolution|\newline
 Only affects level placement and defines the maximum normalized
 velocity that should be resolved by the grid. Experience showed that
 values of $2$ - $3$ are sufficient for reconstruction of the
 generalized Spitzer function up to $5$ times the thermal velocity.
 \item \verb|collop_base_prj|\newline
 Projection base for basis function expansion.
 \begin{itemize}
  \item 0: Generalized Laguerre polynomials of order $3/2$ (Default).
  \item 1: Standard polynomials $\phi_m(x) = x^m$.
  \item 2: Quadratic polynomials $\phi_m(x) = x^{2m}$.
  \item 10: Cubic Splines generated from a $y_m = (0, 0, ..., 1, ..., 0)$ grid.
  \item 11: General B-Splines (best choice).
 \end{itemize}

 \item \verb|collop_base_exp|\newline
 Expansion base for basis function expansion. See \verb|collop_base_prj|
 for parameters. At the moment it is only tested for
 \verb|collop_base_prj = collop_base_exp|.

 \item \verb|collop_bspline_order|\newline
 According to the B-Spline definition this is the order parameter $k$.
 As an example $k=3$ creates quadratic B-Splines and $k=4$ cubic
 B-Splines (best choice).

 \item \verb|collop_bspline_dist|\newline
 This parameter was introduces for testing a non-uniform knot
 distribution for B-Splines. Default value is $1$ which defines a
 uniform knot distribution (best choice).

 \item \verb|phi_x_max|\newline
 Important parameter for numerical integration and definition of
 B-Spline knot distribution. The B-Splines are distributed between
 $x=0$ and this value. A typical choice is $5$. Above this value the
 B-Splines are extrapolated with a Taylor series.

 \item \vv{mag_write_hdf5}\newline
 Creates \vv{magnetic.h5} which contains all information of the magnetic
 field as it is ``seen'' from \vv{NEO-2}.

 \item \vv{lsw_save_dentf}, \vv{lsw_save_enetf}, \vv{lsw_save_spitf}\newline
 Defines if the gradient driven distribution function (mainly used for
 bootstrap studies) or the generalized Spitzer function is stored. These
 settings of course require \vv{NEO-2} reconstruction runs.

 \item \vv{prop_reconstruct}\newline
 0 means a standard \vv{NEO-2} run. If \vv{prop_write = 2}, then all
 information for subsequent reconstruction runs are stored. For full
 reconstruction \vv{NEO-2} has to be run all reconstruction steps from 0 to
 3, where 3 is a service run which cleans up the directory and merges
 HDF5 files of this run. Note that for the parallelized stellarator version only
 reconstruction steps 0 and 2 can be parallelized with MPI.

\end{itemize}

% ----------------------------------------------------------------------
\subsection{Preparation of equilibrium\label{running_preparation_equilibrium}}
The main directory for everything related to ECCD is
\vv{NEO2DATAPATH/Interface/}.
Equilibrium magnetic field in Boozer coordinates is provided for NEO-2
in the form of text files called in this document ``Boozer files'', with usual file ending ``bc''.
They contain data in a specific format (meant here: not just a table of values).
Examples can be found in \vv{DOC/example_bg_field.bc} and \vv{DOC/example_pert_field.bc}.
The Boozer files for ECCD runs are
located in
\begin{verbatim}
$NEO2DATAPATH/Interface/Boozer/
\end{verbatim}

Usually the W7-X files are already in the correct format to be read by
\vv{NEO-2} despite of some additional comment lines at the beginning of the
file which can be safely removed. An example of the original file and
the slightly modified file (some comments have been removed) is given
here: \vv{NEO2DATAPATH/Interface/Boozer/w7x-m111-b3-i1/}.

% ----------------------------------------------------------------------
\subsection{Preparation of the plasma parameter profiles\label{running_preparation_profiles}}
The profiles for ECCD can be found here:
\vv{NEO2DATAPATH/Interface/Profiles/}. Profiles are usually
provided as text files, e.g.
\begin{verbatim}
$NEO2DATAPATH/Interface/Profiles/w7x-m111-b3-i1/prf.txt
\end{verbatim}
You may also get them directly from a database.
These
plasma parameter profiles are used for computation of the collisionality
parameter $\kappa$, which is then used as an input to \vv{NEO-2}. Please note
that the collisionality is a function of the flux surface label. For the
converter is it necessary that the file has the appropriate format and
the that units are correct. A MATLAB script is given in the file
\vv{convert_prf.m}, while it should be noted that the reader section of
this script is adapted for one particular input file format. The output
file has to have one line per flux surface, with four columns each:
\begin{itemize}
 \item 1. Column: Normalized toroidal flux (\vv{boozer_s}).
 \item 2. Column: Electron density in cm$^{-3}$.
 \item 3. Column: Electron temperature in eV.
 \item 4. Column: Effective charge $Z_\mathrm{eff}$.
\end{itemize}

% ----------------------------------------------------------------------
\subsection{Preparation of the run directories\label{running_preparation_directories}}
Next step is the preparation of the flux surface grid for the \vv{NEO-2}
runs. It is started from an uniform grid where a given radial grid point is
slightly shifted if a low-order rational flux surface is detected.
Afterwards the given profile file is read for computation of the
collisionality parameter (by spline interpolation along the radial
coordinate).

It is now necessary to copy the following required files to a new
empty directory (the working directory),
\begin{verbatim}
boozer file
profile file
neo.in configuration file
create_surfaces.in configuration file
ln -s $NEO2PATH/tools/create_surfaces/Build/create_surfaces.x
\end{verbatim}
(You have to replace ``Build'' by the name of the build directory you
used.)

The namelist file \vv{create_surfaces.in} is described as follows:
\begin{itemize}
 \item \vv{s_beg} \newline
 First flux surface (normalized toroidal flux)
 \item \vv{s_end} \newline
 Last flux surface (normalized toroidal flux)
 \item \vv{s_steps} \newline
 Number of flux surfaces
 \item \vv{mag_nperiod_min} \newline
 Minimum number of toroidal field periods
 \item \vv{mag_nperiod_max} \newline
 Maximum number of toroidal field periods (required for avoiding
 low-order rational flux surfaces)
 \item \vv{output_file} \newline
 Output file with input parameters for \vv{NEO-2}
 \item \vv{isw_create_surfaces} \newline
 Defines if flux surfaces should be found for \vv{NEO-2} runs or if only the
 plasma parameter profiles should be splined (Default: true)
 \item \vv{profiles_file} \newline
 Input file for plasma parameter profiles (as it was generated in the
 last step)
\end{itemize}

Running \vv{./create_surfaces.x} creates a grid in \vv{boozer_s} that
fulfills the properties as defined in the configuration file. Afterwards a
spline interpolation is used to compute the collisionality parameter per
flux surface. The file \vv{surfaces.dat} (as defined by \vv{output_file}
in the configuration file) is structured as follows:
\begin{itemize}
 \item 1. Column: \vv{boozer_s}
 \item 2. Column: \vv{conl_over_mfp} (negative because this is $\kappa$
 as used in \vv{NEO-2})
 \item 3. Column: \vv{Z_eff}
 \item 4. Column: \vv{T_e} (Electron temperature in eV)
\end{itemize}

Please note that the code for creating the surfaces allows for some
surfaces to have more periods than defined in \vv{mag_nperiod_max} in
order to not shift this surface to far away from the original
equidistant grid.

In the next step a template directory for the run directories has to be
created. Here, a default \vv{neo2.in} is used (so be sure to chekc the
settings) and the executable of
\vv{NEO-2} is copied from the project directory.
\begin{verbatim}
mkdir TEMPLATE_DIR
cd TEMPLATE_DIR
cp ../BOOZER_FILE_NAME.bc .
cp ../neo.in .
cp $NEO2PATH/NEO-2-PAR/Build/neo_2.x .
cp $NEO2PATH/DOC/neo2.in.par-full ./neo2.in
cp $NEO2DATAPATH/Interface/Examples/condor-all.submit .
\end{verbatim}

The last line is only used if HTCondor should be used as batch system.
An alternative approach is the condor\_submit\_example file in the
\vv{DOC} folder of \vv{NEO-2}.

In file \vv{neo2.in} the values of the four quantities
\vv{boozer_s} (in namelist settings), \vv{conl_over_mfp}, \vv{T_e} and
\vv{z_eff} (in namelist collision) have to be set to their name (with
case as given here) in \vv{< >} brackets. These serve as placeholders
that are replaced in the following step.

Next, the code
\vv{\$NEO2PATH/PythonScripts/create_surf.py} has to has to
be started, in the working directory.

This creates \vv{s_steps} directories, each with an modified
\vv{neo2.in} file. It is recommended to change into one of these
directories and to check if \vv{boozer_s}, \vv{conl_over_mfp},
\vv{z_eff} and \vv{T_e} have been set correctly. Note that the latter is
only used by \vv{NEO-2} in the relativistic case, i.e. \vv{isw_relativistic > 1}.

The Python script has an output representing the directory names that
have been created. This output is required by some other tools, and thus
should be copied into a file named \vv{jobs_list.txt}. You can achieve
this by using either of the commands
\begin{verbatim}
./create_surf.py > jobs_list.txt
\end{verbatim}
or
\begin{verbatim}
./create_surf.py | tee jobs_list.txt
\end{verbatim}
instead of what is given above. We recomend the latter, as the output is
written to file and to the screen and thus allows you to check the
progress.

\subsubsection{Notes on resources}
The condor submit files determine the resources requested by the job in
the two fields \vv{request_cpus} (note that this number should match
the number for \vv{-np} in \vv{arguments}) and \vv{request_memory}.
The amount of memory required is fixed by the parameters of the jobs
(you may still request more to make sure they run on a specific subset
of nodes, but there are other options for this).
The number of processors can be choosen freely up to the number of
processors available per node, or even more, but this is untested and
may reduce performance due to limited bandwidth between nodes.

% ----------------------------------------------------------------------
\subsection{Running the jobs\label{running_running}}
In order to start the jobs the command
\begin{verbatim}
run_condor.sh condor-all.submit
\end{verbatim}
can be used (\vv{run_condor.sh} is part of \vv{NEO-2}), which submits a
condor job for each directory given in
\vv{jobs_list.txt}. Checking the output of \vv{condor_q} shows the
number of submitted Condor jobs.

Another approach would be to use a condor submit file similar to
\vv{condor_submit_example} in the DOC folder of the \vv{NEO-2} repository.
While the above will create one ``batch'' per subfolder, the
example file would create one ``batch'' in total, which has one job per
subfolder starting with \vv{es_}.
It can be used with
\begin{verbatim}
condor_submit condor_submit_example
\end{verbatim}

In either case (\vv{run_condor.sh} and \vv{condor_submit}), this runs
only \vv{prop_reconstruction = 0} (see entry in
sec.~\ref{running_preparation_neo2in}).
You have to rerun each folder also for 1,2 and 3 (3 can be replaced with
python3 function \vv{prop_reconstruct_3} of module \vv{hdf5tools}).
Changing the value can be done with the script \vv{switch_reconstruction.sh}
for a single file, or with python3 function
\begin{verbatim}
from scan_nonhdf5_tools import set_neo2in_reconstruction
set_neo2in_reconstruction(folder='./',
                          subfolder_pattern='s[123456789].*',
                          backup=False,
                          value=1)
\end{verbatim}
for all files.

%~ An alternative is the python script \vv{neo_2_par_wrapper.py} as
%~ executable for condor, e.g.
%~ \begin{verbatim}
%~ Executable = ./neo_2_par_wrapper.py
%~ arguments = -np  4
%~ request_cpus = 4
%~ \end{verbatim}
%~ for a run which should use four processors.

Once the jobs have finished \vv{prop_reconstruction = 3}, the code
\begin{verbatim}
h5merge.x
\end{verbatim}
needs to be run. It can be found in \vv{libneo} in the tools subfolder.
This code merges the output file of \vv{NEO-2} \vv{final.h5}
of each directory defined in \vv{jobs_list.txt} into one output file.
This output is file is already the one which is used by the \vv{NEO-2}/TRAVIS
interface.

There is also a python3 function as alternative
\begin{verbatim}
import hdf5tools
hdf5tools.copy_hdf5_from_subfolders_to_single_file(path = './',
  infilename = 'final.h5',
  outfilename = 'collected_final.h5',
  only_base_path = True,
  source_base_path = True)
\end{verbatim}
This function does not use file \vv{jobs_list.txt}, but instead checks
each folder for ``infilename'', and ignores folders that do not contain
a file of this name.

In either case the operation is probably limited by filesystem operations
and thus may take a while (at ITCP of TU Graz is a gigabit network and speed is
about 30-40 MB/s, with respect to output file, i.e. an output file size
of 350 GB means this step will take approximately ten thousand seconds).

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Studies of the generalized Spitzer function\label{spitzer_function}}
The following works with the merged output of multiple flux surfaces or
with the output of a single flux surface.

The best option is to create a plot-directory inside of the run
directory of \vv{NEO-2}:
\begin{verbatim}
mkdir PLOTS
cd PLOTS
ln -s $NEO2DATAPATH/Interface/Spitzer_Interface/Build/neo2_g.x
cp $NEO2DATAPATH/Interface/Examples/g_vs_lambda.in .
cp $NEO2DATAPATH/Interface/Examples/spitzerinterface.in .
./neo2_g.x
\end{verbatim}

The config files are defined in the technical documentation of the
interface\footnote{\vv{NEO2DATAPATH/Interface/Documentation/Internal/Interface.pdf}}.
The result file (\vv{g_vs_lambda.h5}) can be plotted with MATLAB, e.g.,
with \vv{plot_g_xfix.m} from the main directory of all MATLAB scripts
related to ECCD:
\begin{verbatim}
$NEO2DATAPATH/Interface/Matlab
\end{verbatim}

Please note that the information about the magnetic field is stored per
\vv{NEO-2} run in \vv{magnetics.h5} (only NEO-2-PAR has support for this).
This file can also be used to plot the field module along the field line
for visualization of the observations points. A useful MATLAB script is
\vv{plot_levels_pgf.m} which also exports the correct format for
\vv{g_vs_lambda.in}.

Further plots of the magnetic field, such as a 2D distribution of the
magnetic field module on a particular flux surface can be made with the
tools located here:
\begin{verbatim}
$NEO2DATAPATH/Interface/Magfie
\end{verbatim}

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Running TRAVIS\label{travis}}
TRAVIS is located here:
\begin{verbatim}
/proj/plasma/TRAVIS
\end{verbatim}
In order to start the GUI the following steps are necessary:
\begin{itemize}
 \item \vv{/proj/plasma/TRAVIS/TRAVIS-16-04-08/bin/bin_ECRH/travisGUI64_IOTA}
 \item Say yes to the question if the last project should be loaded.
 \item Preferences -> Advanced -> Expert mode
 \item On the top right corner select the TRAVIS kernel for your demands
 (SYNCH, \vv{NEO-2} non-rel, \vv{NEO-2} rel, pure TRAVIS)
\end{itemize}

The configuration with HDF5 input file for the interface is used has to
be set in the usual \vv{spitzerinterface.in} file which is located in
the RUN/TEMP directory of TRAVIS. This can also be selected in the
settings of TRAVIS. If there is an upgrade to the \vv{NEO-2} interface or to
SYNCH, the TRAVIS kernels have to be recompiled. This has to be done
here:
\begin{verbatim}
/proj/plasma/TRAVIS/TRAVIS-16-04-08/TRAVIS-src
\end{verbatim}
with the command \vv{make gnu}. However, before several config files
have to be set, especially, \vv{makeunix/mkfiles/defs.mk}.

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Bootstrap\label{bootstrap}}
For plotting the bootstrap coefficient it is not necessary to run \vv{NEO-2}
in the reconstruction mode. Plotting can be done with the script
\vv{plot_evolve_h5.m}.
This script has to be started in the directory where the complete \vv{NEO-2}
run is located.

For investigating the gradient driven distribution it is necessary to
complete a full reconstruction run of \vv{NEO-2} (steps 0 to 3).
Once the run(s) is(are) finished, plotting can be done with the
MATLAB/octave scripts provided with \vv{NEO-2},
in order to evaluate the antisymmetric part of the gradient driven
distribution function at various observation points. The script
\vv{count_maxima_bootstrap.m} is a good place to start with.

This script reads the file \vv{magnetics.h5} from the run directory of
\vv{NEO-2} and shows a part of the field line. The variable \vv{point_phimfl}
defines the angle along the field line defining the observation point
indicated as dot in the figure. The MATLAB script has an output on the
console of the following format:
\begin{verbatim}
                  s              theta                phi     tag
     2.50000000e-01     4.43238764e+00     5.98323740e+00      p1
\end{verbatim}

This is the input for the \vv{NEO-2} interface computing the distribution
function at the defined observation point(s).

A good start for beginning is with LHD runs for the bootstrap tasks, e.g.:
\begin{verbatim}
$RUNPATH/
conl_over_mfp_5m5-boozer_s_0.25d0-mag_nperiod_min_1-bsfunc_local_err_1m1
\end{verbatim}
Note that a newline was added in the above path, to improve readability.
In the subdirectory \vv{PLOTS} a symbolic link to the distribution
function plotter for bootstrap is already prepared
(\vv{NEO2DATAPATH/Interface/Spitzer_Interface/Build/dentf_lorentz.x}).
Please note that this executable is slightly different to \vv{neo2_g.x}
as used for Spitzer function, because a significant higher number of
$x$-values (velocity module) are used for plotting. In
\vv{g_vs_lambda.in} the two lines from the output of the MATLAB script
should be inserted. Next, \vv{./dentf_lorentz.x} can be run in the
\vv{../PLOTS} directory. This produces \vv{g_vs_lambda.h5} (as before
for ECCD tasks). Finally, the MATLAB script from before can be run a
second time and the distribution function including the relevant maxima
is plotted.

Please note, that if the observation point is changed,
\vv{./dentf_lorentz.x} has to be run again with an updated input file.
Otherwise, the MATALB plotter will show data from the previous run of
the interface.

The settings in \vv{spitzerinterface.in} are described in the interface
documentation. It should be noted that the parameter \vv{plot_source}
can have the values \vv{dentf}, \vv{enetf}, and \vv{spitf} defining the
distribution function to be plotted.

% ----------------------------------------------------------------------
\subsection{Scripts for running the jobs in Condor}
In the \vv{ShellScripts} subfolder of \vv{NEO-2}
are two important scripts which are useful for running many
instances of \vv{NEO-2} at the same time via Condor.

The script \vv{create_dirs_paramvalue.sh} reads a file called
\vv{dirs_list.txt} and copies from a directory \vv{TEMPLATE_DIR} all
files to given subdirectories and modifies each \vv{neo2.in}. It is best
explained by looking at some examples, e.g., here:
\begin{verbatim}
$RUNPATH/RunsByDate__2017_07_Bootstrap__ratfieldline
\end{verbatim}

For running the jobs the same script can be used as introduced in the
ECCD section of this writeup (\vv{run_condor.sh}). Here, the content of
the condor submission file is described in more detail:
\begin{verbatim}
Executable = NEO2-all.sh
Universe   = vanilla

Error      = NEO2-0.e$(Cluster)
Log        = NEO2-0.l$(Cluster)
Output     = NEO2-0.o$(Cluster)

notification = Never

request_cpus   = 4
request_memory = 30 * 1024

Getenv     = true
should_transfer_files = NEVER

run_as_owner = true
#requirements = (TARGET.Machine != "faepop02") && (TARGET.Machine != "faepop03")
#environment = "XDG_CACHE_HOME=/tmp"

Queue
\end{verbatim}
The first line defines the executable which has to be located in
\vv{/temp/} because Condor does not have access to AFS. The Universe
defines the Condor environment and is always vanilla at our institute.
The next three lines define the file names for the output files, where
(\$Cluster) will be an ID which is created by Condor defining the job.
The notification line defines if a mail is sent when the job has
finished.

The launch script \vv{NEO2-all.sh} (located in the template directories)
is written in a way that MPI is used for running \vv{NEO-2}. Here, the number
of processors used is given by \vv{request_cpus} in the Condor
submission file. In the vanilla universe it is not possible to run the
MPI jobs across machines so that a typical number is 4 to 8 at our
institute. The requested memory (30*1024 MB = 30 GB) defines how much
memory the job will be allowed to use (all MPI processes together).
If Condor jobs are failing then one should look into the
file NEO-2.e(\$SomeNumber) because there the error messages are located.
Some IEEE underflow errors are "normal" at the moment. They arise from
some underflow during the integration of the matrix elements and might
be fixed in the future.

The \vv{requirements} and \vv{ernvironment} lines are commented, i.g. not actually
considered in this example.
The \vv{environment} needs to be set on the machines of the Institute of
Theoretical and Computational Physics at TU Graz.
The requirements do not have to be changed except one would like
to exclude a special machine from Condor.
If your jobs fits into the memory of regular machines, then you should
exclude the high memory machines (e.g. faepop12 and faepop13), so as to
not block them for people who need the memory.

% ----------------------------------------------------------------------
\subsection{Boozer file for rational field line}
For creating a magnetic equilibrium with toroidal perturbations, the
MATLAB script
\begin{verbatim}
boozer_rational.m
\end{verbatim}
can be used.

This script starts from \vv{tok-synch2.bc}, adds a perturbation field
with given mode number and perturbation amplitude. In addition, a flat
rotational transform profile can be created.

Running the script \vv{plot_evolve_h5.m} in the parent directory
containing the run directories, plots the bootstrap coefficient. Using
\vv{plot_levels.m} in a
specific run directory (containing \vv{magnetics.h5}) plots the magnetic
field module along the field line with level distribution.

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Running NEO-2 on Marconi cluster\label{marconi}}
While in general the steps to run \vv{NEO-2} are the same, there are
some points that differ. Those are covered here for running the relativistic version of \vv{NEO-2}
on the Marconi cluster, as well as some additional hints.

Be sure to also take a look at the documentation
\begin{verbatim}
https://wiki.u-gov.it/confluence/display/SCAIUS/HPC+User+Guide
\end{verbatim}

% ----------------------------------------------------------------------
\subsection{Getting data to/from Marconi cluster}
Note that at the time of writing using \vv{scp}/\vv{rsync} on Marconi to
TU Graz does not work, i.e. you have to run these commands in Graz.
You can check if you might suffer from the same problem by using \vv{ping}:
\begin{verbatim}
ping localadress
\end{verbatim}
if the packages arrive, then \vv{scp}/\vv{rsync} work.

% ----------------------------------------------------------------------
\subsection{Preparations}
First you will need a user account for cineca, then acess to the HPC
system. The necessary steps will not be repeated here, check the
documentation for this (see link above).

As other HPC environments Marconi uses a module system, to provide
different versions of software (e.g. compiler).

The following list of modules was used, and loaded in \vv{.bashrc}

\begin{verbatim}
module load cmake/3.18.2
module load python/3.6.4

# hdf5 requirements
module load szip/2.1--gnu--6.1.0
module load zlib/1.2.8--gnu--6.1.0

#######################################################
## Intel configuration
module load intel/pe-xe-2018--binary
module load blas/3.8.0--intel--pe-xe-2018--binary
module load lapack/3.8.0--intel--pe-xe-2018--binary
module load metis/5.1.0--intel--pe-xe-2018--binary

## following two are scipy requirements
module load mkl/2018--binary
module load numpy/1.14.0--python--3.6.4
module load scipy/1.2.2--python--3.6.4

#######################################################
## gfortran configuration
module load gnu/7.3.0
module load openmpi/3.0.0--gnu--7.3.0
\end{verbatim}

The intel compiler is loaded because it is a  prerequisite for \vv{blas}
and \vv{lapack}, for compiling the libraries the gnu compiler was used.

It is also recommended to set an alias for easy changing to the scratch
folder and for restricting the output of \vv{slurm} queue command, e.g.
\begin{verbatim}
alias cdscratch='cd ${CINECA_SCRATCH}'
alias queue='squeue --user=${USER}'
\end{verbatim}
where the latter command would give you only your queued jobs, not those
of all users.

Additionally these libraries had to be compiled:
\begin{itemize}
  \item hdf5
  \item gsl
  \item fgsl
  \item SuiteSparse
\end{itemize}

Note that there is a gsl module available, but for unknown reasons it
was not found during configuration.
The hdf5 library had only intel compiled versions available, which we
can not use, as we need the fortran module files, which are compiler
(and version) dependent.
Note that the same holds for the mpi library and for \vv{fgsl}, while
for \vv{gsl} and \vv{SuiteSparse} this should not be the case.

Matching releases of \vv{hdf5}, \vv{gsl} and \vv{fgsl} have been
downloaded localy and then copied to Marconi.

For the libraries shell scripts \vv{buildcommands.sh} have been written.
Those of \vv{hdf5} is
\begin{verbatim}
#! /bin/bash

BUILDFOLDER=Build-7.3.0

mkdir -p $BUILDFOLDER
cd $BUILDFOLDER

CC=/cineca/prod/opt/compilers/openmpi/3.0.0/gnu--7.3.0/bin/mpicc \
  FC=/cineca/prod/opt/compilers/openmpi/3.0.0/gnu--7.3.0/bin/mpif90 \
  ../configure --prefix=$HOME/local/ --enable-fortran --enable-parallel --enable-build-mode=production

make
make install

cd ..
\end{verbatim}
If the c and fortran compilers where not set explicitly, then
configuration failed with the message, that no working mpi compiler
could be found.

Those of \vv{gsl} is
\begin{verbatim}
#! /bin/bash

BUILDFOLDER="Build-7.3.0"

mkdir -p $BUILDFOLDER
cd $BUILDFOLDER

CC=/cineca/prod/opt/compilers/openmpi/3.0.0/gnu--7.3.0/bin/mpicc \
  FC=/cineca/prod/opt/compilers/openmpi/3.0.0/gnu--7.3.0/bin/mpif90 \
  ../configure --prefix=${HOME}/local/
make
make install

cd ..
\end{verbatim}
Those of \vv{fgsl}
\begin{verbatim}
#! /bin/bash

BUILDFOLDER=Build-7.3.0

mkdir -p $BUILDFOLDER
cd $BUILDFOLDER

export gsl_LIBS=`gsl-config --libs`
export gsl_CFLAGS=`gsl-config --cflags`

CC=/cineca/prod/opt/compilers/openmpi/3.0.0/gnu--7.3.0/bin/mpicc \
  FC=/cineca/prod/opt/compilers/openmpi/3.0.0/gnu--7.3.0/bin/mpif90 \
  ../configure --prefix=${HOME}/local/
make
make install

cd ..
\end{verbatim}
but before this can be used, once the commands
\begin{verbatim}
mkdir m4
autoreconf -i
\end{verbatim}
are required.

SuiteSparse was cloned from its git repository and then compiled
\begin{verbatim}
git clone https://github.com/DrTimothyAldenDavis/SuiteSparse.git
make BLAS="-L${MKLROOT}/lib/intel64/ -lmkl_rt"
\end{verbatim}
Note that not all parts could be comiled succesfully, but as the required
parts could be build, this was not further investigated. (\vv{SPQR}
did not work because of compiler version.)

To make sure \vv{gsl} and \vv{fgsl} are found, the following lines
where added to the \vv{.bashrc}
\begin{verbatim}
export PATH=${HOME}/local/bin/:${PATH}
export LD_LIBRARY_PATH=${HOME}/local/lib/:${LD_LIBRARY_PATH}
export PKG_CONFIG_PATH=${HOME}/local/lib/pkgconfig/:${PKG_CONFIG_PATH}
\end{verbatim}

\subsection{Compiling libneo and NEO-2}
Some specific modifications had to be made to get \vv{libneo} and
\vv{NEO-2} to compile on Marconi.

For \vv{libneo} the line
\begin{verbatim}
target_link_libraries(libneo lapack)
\end{verbatim}
in \vv{CMakeLists.in} had to be commented.
Additionally the bodies of the routines \vv{h5_add_complex_1} and
\vv{h5_add_complex_2} in \vv{src/hdf5_tools/hdf5_tools.f90} had to be
commented. This is because the version of the  compiler, does not
allow to use \vv{complex\%re} or \vv{complex\%im} (The
slash \vv{\\} had to be added due to \LaTeX).
This should not be a problem for \vv{NEO-2} as so far only real values
need to be read.

For \vv{NEO-2}, in \vv{ProjectConfig.cmake.in} the lines
\begin{verbatim}
set(SUITESPARSE_INCLUDE_DIR_HINTS
    /marconi/home/userexternal/rbuchhol/Programs/SuiteSparse/include/)
set(SUITESPARSE_LIBRARY_DIR_HINTS
    /marconi/home/userexternal/rbuchhol/Programs/SuiteSparse/lib/)
\end{verbatim}
have been added to make sure the \vv{SuiteSparse} library is found.
The setting of PROJECTLIBS and MPE\_PATH have been commented, while
setting of LIBNEOLIBS has been changed to
\begin{verbatim}
set(LIBNEOLIBS /marconi/home/userexternal/rbuchhol/Programs/libneo/
    CACHE STRING "libneo library path")
\end{verbatim}
Final change was adding a elseif-clause of the compiler-id check.
\begin{verbatim}
  elseif (CMAKE_Fortran_COMPILER_VERSION VERSION_EQUAL 7.3.0)
    set(FGSL_PATH /marconi/home/userexternal/rbuchhol/local/)
    set(NEO2_Libs ${LIBNEOLIBS}/Build-7.3.0/ CACHE STRING "Libneo path")
\end{verbatim}

In \vv{CMakeSources.in} the lines corresponding to SuperLU have been
commented, as it was decided to compile only one of SuiteSparse and SuperLU.

In \vv{CMakeLists.txt} lines
\begin{verbatim}
include_directories(${FGSL_INC})
include_directories(${HDF5_Fortran_INCLUDE_DIRS})
\end{verbatim}
have been added (with other include\_directory calls).
The lines correspondig to SuperLU have been commented.
Searching for gsl is changed to (from find\_library)
\begin{verbatim}
find_package(GSL COMPONENTS GSL_LIBRARY GSL_CBLAS_LIBRARY REQUIRED)
\end{verbatim}
Finally in \vv{target_link_libraries} the entries for \vv{gsl}, and
\vv{fgsl} are changed
\begin{verbatim}
${fgsl_lib}
${gsl_lib}
${GSL_LIBRARIES}
\end{verbatim}

In \vv{COMMON/sparse_mod.f90} the body of the \vv{SuperLU} subroutines
have been commented.

\subsubsection{Additional hints}
The intel compiler has been used to compile \vv{libneo} and \vv{NEO-2},
but simulations did only run with one processor, with more, no work was
actually done. It could be verified, that the \vv{initMaster} subroutine
of the \vv{neo2scheduler_module} was not called, but those of the base
class. The reason for this is unknown, and as there was neither an idea
how to fix this, the gnu compiler was used instead.
Note that this but was the reason for the introduction of \vv{test_mympilib}.
At the time of writing this, it failed with the intel compiler on Marconi,
but worked with the gnu compiler, also at the ITCP at TU Graz.

When using the intel compiler, if you get an error or the form
\begin{verbatim}
/PATH/file.f90(line): error #7002: Error in opening the compiled module file. &
  &  Check INCLUDE paths. [HDF5]
  USE hdf5_tools
\end{verbatim}
(same has been seen with \vv{[GSL]} and \vv{use fgsl}), then the problem
is not that \vv{hdf5_tools} (or the modules) is not found, the problem
is that \vv{hdf5} (or the modules) is not found.
Note that a line break has been added, denoted with \& at the end of the
line and the beginning of the continuation line.

Error messages of the kind
\begin{verbatim}
PATH/sparse_mod.f90.o: In function `sparse_mod_mp_sparse_solve_superlucomplex_b1_':
sparse_mod.f90:(.text+0xbc5a): undefined reference to `c_fortran_zgssv_'
sparse_mod.f90:(.text+0xc540): undefined reference to `c_fortran_zgssv_'
sparse_mod.f90:(.text+0xcdcf): undefined reference to `c_fortran_zgssv_'
\end{verbatim}
mean that there are still references to \vv{SuperLU} routines, either
comment them or also compile SuperLU.

% ----------------------------------------------------------------------
\subsection{Submitting a job}
In the previous chapters the required libraries and \vv{NEO-2} have been
compiled, so now it is time to submit a job.
On the Marconi cluster the \vv{slurm} workload manager is used, which
comes with a variety of tools. For submiting a job \vv{sbatch} is used.
While it is possible to supply job requirements directly as parameter to
\vv{sbatch}, we recommend to supply them with a script. A commented
example can be found in the \vv{DOC} folder of \vv{NEO-2}, here we give
just a basic example, together with some hints.
\begin{verbatim}
#!/bin/bash

#SBATCH --job-name=NEO-2-W7X
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=24
#SBATCH --ntasks-per-socket=12
#SBATCH --cpus-per-task=1
#SBATCH --time=24:00:00
#SBATCH --partition=skl_fua_prod
#SBATCH --account=FUA35_TSVVSTOP
#SBATCH --err=%x.err
#SBATCH	--out=%x.out

export OMP_NUM_THREADS=1

sed -i "s#prop_reconstruct.*=.*#prop_reconstruct = 0#" neo2.in
srun --cpu-bind=cores ./neo_2.x

sed -i "s#prop_reconstruct.*=.*#prop_reconstruct = 1#" neo2.in
srun -n1 ./neo_2.x

sed -i "s#prop_reconstruct.*=.*#prop_reconstruct = 2#" neo2.in
srun --cpu-bind=cores ./neo_2.x

sed -i "s#prop_reconstruct.*=.*#prop_reconstruct = 3#" neo2.in
srun -n1 ./neo_2.x
\end{verbatim}
This example can be run with
\begin{verbatim}
sbatch filename
\end{verbatim}
As should be clear from the first line of the example this is basically a \vv{bash}
script, with two special points. First are the lines starting with \vv{\#SBATCH}
which indicates to \vv{sbatch} that there follows a command. Possible
commands are the same that could be given on the commandline (check also
the help of \vv{sbatch}). Second, instead of \vv{mpirun} (or something
similar) \vv{srun} is used.

What does this script? It will do all four stages of a single flux
surface. Stages 0 and 2 are parallelized, the other two not.

Lets go through the file in a bit more detail. Job name should be clear,
the next four commands refer to the number of processors. At the time of
writing this, a common composition is this: basic unit is a thread, which
runs on one or more (physical) processors or cores or cpus. Many cores
together form a chip, which sits in a socket. One or two sockets form a
node (comparable to one motherboard).
Hopefully this makes the commands a bit clearer. We request a single
node, and want to start 24 threads on it, with half on each socket
(Marconi has two chips per node). Each thread should use only one core
(thus wasting half of them).
The time parameter is the time we want to allow the job to run. The job
will be canceled if it does not finish within the time limit.
The two parameters partition and account are cluster specific. The
former specifies on which part of the cluster the job should run (and
thus which rules apply, like maximum number of nodes and time). The
account means not a user account but a time billing account: from
which projects time budget should the used time be substracted.
The final two \vv{sbatch} lines define file names for error and standard
output. We want to match the output filenames to match the job name, and
thus make use of predefined variables. Within the \vv{sbatch} arguments
these are indicated with a percent sign followed by a letter.

Lines with \vv{\#SBATCH} are only processed until something other
than a comment or an empty line is found.

The export makes sure no additional \vv{OpenMP} parallelization is used.

We make sure that \vv{prop_reconstruct} has the required value with the
\vv{sed} command(s).
After each of these, we start \vv{NEO-2}. The zeroth and second step
have the additional parameter \vv{cpu-bind=cores}. The Marconi
documentation recomends using these, when fewer cores than available are
used. Note that this is a parameter of \vv{srun} and not \vv{sbatch},
which is why it appears here an not at the top.
The first and third step, overwrite the total number of tasks with
\vv{-n1}, as these should not run in parallel.

\subsubsection{Submitting multiple jobs}
If you want to submit multiple jobs at once, i.e. doing a scan over flux
surfaces, there are two possibilities.

First you can have one \vv{slurm} batch file in each folder, and then submit
each as a single job, either by hand or with a script.
The slurm documentation does recommend not to use \vv{sbatch} in a loop.
An easy way to get the \vv{slurm} batch file into each folder is to put
it into the \vv{TEMPLATE_DIR} before running \vv{create_surf.py}.
The script will then copy it into each folder along with the other files.

The second option is to make use of the \vv{array} parameter of
\vv{sbatch}. This is somewhat similar to parallelization with \vv{MPI},
in the sense that you have to distinguish what each ``thread'' does, by
its index. If you use folder names, that include numbering, this is easy
to do with the means provided by \vv{sbatch}. With the foldernames as
created by python script \vv{create_surf.py}, this is a bit more
complicated, but doable. Add/replace the following lines/parts in the
above \vv{slurm} example file, in the appropriate places.
\begin{verbatim}
#SBATCH --array=1-30
#SBATCH --err=%x_%A_%a.err
#SBATCH	--out=%x_%A_%a.out

folder=$(ls -d s[123456789].* | sed -n ${SLURM_ARRAY_TASK_ID}p)
cd ${folder}
\end{verbatim}
The first line determines we want to submit 30 copies of the script,
with indices from one to 30. Note that starting at one is important.
The output filenames are modified to also include the main job id and
the task id (array index).
The last two lines first determine which folder to use, and then change
into the folder. Determining the folders is a two step process. First
we generate a list of all folders, the \vv{-d} is important, otherwise
\vv{ls} would generate a list which also contains all the content of the
folders. In the second step we use \vv{sed} to print only those entry,
that matches out array index. As \vv{sed} uses a one based index, it is
important to also use a one based index for the array.
This will then run \vv{NEO-2} in each of the folders, but the \vv{slurm}
output files will still be in the working directory.
Note that it is also not recomended to submit large arrays.

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{List of scripts/codes used besides NEO-2}
Here four tables list executables, shell scripts, python scripts and octave/matlab scripts,
respectively, that are mentioned in this document besides \vv{NEO-2}.

Note that \vv{$INTERFACE} is here short for \vv{$NEO2DATAPATH/Interface/Spitzer_Interface/}.

\begin{table}[h]
\centering
\begin{tabular}{l|l}
Name/path   & Comment \\ \hline
tools/create\_surfaces & convert profile to form used by create\_surf.py \\
libneo:tools/h5merge & merges final.h5 of subdirectories \\
\$INTERFACE/Build/neo2\_g.x & \\
\$INTERFACE/Build/dentf\_lorentz.x &
\end{tabular}
\caption{additional executables used besides \vv{NEO-2}}
\label{tab:additionalcodes}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l|l}
Name/path   & Comment \\ \hline
run\_condor.sh & \\
create\_dirs\_paramvalue.sh & \\
?/NEO2-all.sh & wrapper for running with condor
\end{tabular}
\caption{additional shell scripts used}
\label{tab:additionalshellscripts}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l|l}
Name/path   & Comment \\ \hline
create\_surf.py & creates directories from template \\
neo\_2\_par\_wrapper.py & to run all three reconstruction steps
\end{tabular}
\caption{Additional python3 scripts used. If not otherwise noted, they
are in the \vv{PythonScripts/} subfolder of \vv{NEO-2}}
\label{tab:additionalpython3scripts}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l|l}
Name/path   & Comment \\ \hline
boozer\_rational.m & \\
convert\_prf.m & convert profile to form used by create\_surfaces \\
count\_maxima\_bootstrap.m & \\
plot\_evolve\_h5.m & \\
plot\_g\_lamfix.m & plot distribtion function, velocity module fixed \\
plot\_g\_xfix.m & plot distribtion function, pitch angle fixed \\
plot\_levels.m & requires \vv{magnetics.h5}) \\
plot\_levels\_pgf.m &
\end{tabular}
\caption{additional octave scripts used}
\label{tab:additionaloctavescripts}
\end{table}

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{Open tasks}
\begin{itemize}
 \item Include NBI source term.
 \item Distribution function plotter in NEO-2-QL.
 \item Cleanup CMake files and prepare for next major Debian upgrade
 (maybe works already).
 \item Save memory in level placement. (Done, December 2017)
 \item Diffusion coefficients with relativistic collision model in
 NEO-2-PAR. (Done, May 2018)
 \item Adaptive Runge-Kutta solver for EFIT runs. (Done, May 2018)
 \item Different integration accuracy settings needed for single species
 and multispecies run. This needs to be a configuration for the input
 file. (Done, May 2018)
 \item Create magnetic equilibrium where Shaing-Callen limit can be
 reached with \vv{NEO-2}. (Done, June 2018)
\end{itemize}

% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\section{More useful scripts}
\begin{itemize}
 \item \vv{hdf5struct.m} Converts HDF5 file to MATLAB structure. Note,
 that this does not work with octave, but is not required, as \vv{load}
 in octave loads a hdf5 file and converts it to a structure.
 \item \vv{plot_g_xfix.m} Plots the distribution function for fixed
 velocity module.
 \item \vv{plot_g_lamfix.m} Plots the distribution function for fixed
 pitch angle.
 \item \vv{plot_levels.m} Plots the field module along the field line
 with level distribution (needs \vv{magnetics.h5}).
\end{itemize}

%\clearpage
%\renewcommand{\bibname}{References}
%\printbibliography[notcategory=fullcited]

\end{document}
